"""
èŠå¤©API
é›†æˆæ„å›¾ç†è§£å’Œå¯¹è¯ç®¡ç†
"""
from fastapi import APIRouter
from pydantic import BaseModel, Field
from typing import List, Optional
import re
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from backend.app.services.search_service import get_search_service
from backend.app.services.llm_service import get_llm_service
from backend.app.services.question_service import get_question_service
from backend.app.models.conversation import (
    get_conversation_manager,
    ConversationStateEnum
)
from backend.app.utils.hierarchy_util import HierarchyUtil
from backend.app.utils.variant_util import variant_key_for_query

router = APIRouter()


class ChatMessage(BaseModel):
    """èŠå¤©æ¶ˆæ¯"""
    role: str = Field(..., description="æ¶ˆæ¯è§’è‰²ï¼šuser æˆ– assistant")
    content: str = Field(..., description="æ¶ˆæ¯å†…å®¹")


class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚"""
    message: str
    history: Optional[List[ChatMessage]] = Field(default_factory=list)
    logic: Optional[str] = "AND"  # AND or OR
    max_results: Optional[int] = 5
    session_id: Optional[str] = "default"  # ä¼šè¯IDï¼Œç”¨äºå¤šè½®å¯¹è¯


class ChatResponse(BaseModel):
    """èŠå¤©å“åº”"""
    message: str
    results: Optional[List[dict]] = None  # æœç´¢ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
    options: Optional[List[dict]] = None  # é€‰æ‹©é¢˜é€‰é¡¹ï¼ˆå¦‚æœæœ‰ï¼‰
    needs_choice: Optional[bool] = False  # æ˜¯å¦éœ€è¦ç”¨æˆ·é€‰æ‹©
    session_id: Optional[str] = "default"  # ä¼šè¯ID


@router.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    èŠå¤©æ¥å£
    
    é›†æˆæ„å›¾ç†è§£å’Œå¯¹è¯ç®¡ç†
    æ”¯æŒå¤šè½®å¯¹è¯å’Œé€‰æ‹©é¢˜å¼•å¯¼
    """
    # è·å–æœåŠ¡å®ä¾‹
    search_service = get_search_service()
    llm_service = get_llm_service()
    question_service = get_question_service()
    conversation_manager = get_conversation_manager()
    
    # è·å–æˆ–åˆ›å»ºå¯¹è¯çŠ¶æ€
    session_id = request.session_id or "default"
    conv_state = conversation_manager.get_or_create_state(session_id)
    
    # è·å–ç”¨æˆ·æŸ¥è¯¢
    query = request.message.strip()
    if not query:
        return ChatResponse(
            message="è¯·è¾“å…¥æ‚¨è¦æŸ¥æ‰¾çš„ç”µè·¯å›¾å…³é”®è¯ï¼Œä¾‹å¦‚ï¼šä¸œé£å¤©é¾™ä»ªè¡¨é’ˆè„šå›¾",
            session_id=session_id
        )
    
    # æ£€æµ‹ç”¨æˆ·æ˜¯å¦é‡æ–°è¡¨è¾¾éœ€æ±‚ï¼ˆå¦‚"æˆ‘è¦æ‰¾XXX"ã€"æ‰¾ä¸€ä¸‹XXX"ç­‰ï¼‰
    # æ³¨æ„ï¼šä¸è¦å°†"æˆ‘è¦ä¸€ä¸ªXXX"è¯¯åˆ¤ä¸ºé‡ç½®å…³é”®è¯
    reset_keywords = ["æˆ‘è¦æ‰¾", "æ‰¾ä¸€ä¸‹", "æœç´¢", "æŸ¥æ‰¾", "é‡æ–°", "æ¢ä¸€ä¸ª"]
    # æ£€æŸ¥æ˜¯å¦æ˜¯é‡ç½®å…³é”®è¯ï¼ˆæ’é™¤"æˆ‘è¦ä¸€ä¸ª"è¿™ç§æƒ…å†µï¼‰
    is_new_query = False
    if conv_state.state != ConversationStateEnum.INITIAL:
        for keyword in reset_keywords:
            if keyword in query:
                is_new_query = True
                break
        # ç‰¹æ®Šå¤„ç†ï¼š"æˆ‘è¦ä¸€ä¸ªXXX"ä¸åº”è¯¥è§¦å‘é‡ç½®
        if "æˆ‘è¦ä¸€ä¸ª" in query or "æˆ‘è¦ä¸ª" in query:
            is_new_query = False
    
    # å¦‚æœç”¨æˆ·é‡æ–°è¡¨è¾¾éœ€æ±‚ï¼Œé‡ç½®å¯¹è¯çŠ¶æ€
    if is_new_query:
        conv_state.clear()
        # æå–æ–°çš„æŸ¥è¯¢ï¼ˆç§»é™¤é‡ç½®å…³é”®è¯ï¼‰
        for keyword in reset_keywords:
            query = query.replace(keyword, "").strip()
        if not query:
            return ChatResponse(
                message="è¯·è¾“å…¥æ‚¨è¦æŸ¥æ‰¾çš„ç”µè·¯å›¾å…³é”®è¯ï¼Œä¾‹å¦‚ï¼šä¸œé£å¤©é¾™ä»ªè¡¨é’ˆè„šå›¾",
                session_id=session_id
            )
    
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    conv_state.add_message("user", query)
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯é€‰æ‹©é¢˜ç­”æ¡ˆï¼ˆA/B/C/D/E æˆ–å•ä¸ªå­—æ¯ï¼‰
    user_input_upper = query.upper().strip()
    is_option_selection = len(user_input_upper) == 1 and user_input_upper in ['A', 'B', 'C', 'D', 'E']
    
    # å¦‚æœå½“å‰çŠ¶æ€æ˜¯ç­‰å¾…é€‰æ‹©ï¼Œä¸”ç”¨æˆ·è¾“å…¥æ˜¯é€‰é¡¹ï¼Œå¤„ç†é€‰æ‹©
    if conv_state.state == ConversationStateEnum.NEEDS_CHOICE and is_option_selection:
        # è§£æç”¨æˆ·é€‰æ‹©
        if conv_state.current_options:
            # æ‰¾åˆ°å¯¹åº”çš„é€‰é¡¹
            selected_option = None
            for option in conv_state.current_options:
                if option.get('label') == user_input_upper:
                    selected_option = option
                    break
            
            if selected_option:
                # æ·»åŠ ç­›é€‰æ¡ä»¶åˆ°å†å²
                conv_state.add_filter(
                    selected_option.get('type', 'unknown'),
                    selected_option.get('name', '')
                )
                
                # åŸºäºé€‰æ‹©ç­›é€‰ç»“æœ
                option_type = selected_option.get('type')
                option_value = selected_option.get('name')
                
                pre_filter_total = len(conv_state.search_results or [])
                filtered_results = conv_state.search_results
                if option_type == "brand":
                    filtered_results = search_service.filter_by_hierarchy(
                        filtered_results, brand=option_value
                    )
                elif option_type == "model":
                    filtered_results = search_service.filter_by_hierarchy(
                        filtered_results, model=option_value
                    )
                elif option_type == "type":
                    filtered_results = search_service.filter_by_hierarchy(
                        filtered_results, diagram_type=option_value
                    )
                elif option_type == "variant":
                    # è½¦å‹å˜ä½“ï¼šæŒ‰æ–‡ä»¶åå‰ç¼€ç²¾ç¡®åˆ†ç»„ï¼ˆä¾‹å¦‚ â€œä¸œé£å¤©é¾™KL_6x4ç¯å«è½¦â€ï¼‰
                    base = (option_value or "").strip()
                    for suf in (" ç³»åˆ—", "ç³»åˆ—"):
                        if base.endswith(suf):
                            base = base[: -len(suf)].strip()
                            break
                    next_filtered = []
                    for r in filtered_results:
                        k = variant_key_for_query(r.diagram.file_name or "", conv_state.current_query or "")
                        if k and k == base:
                            next_filtered.append(r)
                    filtered_results = next_filtered
                elif option_type == "brand_model":
                    # å“ç‰Œ+å‹å·ç»„åˆï¼šè§£æé€‰é¡¹å€¼ï¼ˆå¦‚"ä¸œé£ å¤©é¾™KL"ã€"ä¸œé£ DOC"ã€"ä¸œé£ VEC"ç­‰ï¼‰
                    brand, model = search_service._parse_brand_model(option_value)
                    if brand and model:
                        # å…ˆæŒ‰å“ç‰Œç­›é€‰
                        filtered_results = search_service.filter_by_hierarchy(
                            filtered_results, brand=brand
                        )
                        # å†æŒ‰å‹å·ç­›é€‰ï¼ˆæ”¯æŒå±‚çº§è·¯å¾„åŒ¹é…ï¼‰
                        if filtered_results:
                            filtered_diagrams = HierarchyUtil.filter_by_model(
                                [r.diagram for r in filtered_results], model
                            )
                            filtered_ids = {d.id for d in filtered_diagrams}
                            filtered_results = [r for r in filtered_results if r.diagram.id in filtered_ids]
                    elif brand:
                        filtered_results = search_service.filter_by_hierarchy(
                            filtered_results, brand=brand
                        )
                
                # æ›´æ–°å¯¹è¯çŠ¶æ€
                conv_state.search_results = filtered_results
                conv_state.current_options = []
                conv_state.option_type = None
                # æ”¯æŒé…ç½®/è½´å‹ç­›é€‰ï¼ˆå¦‚ 6x4 ç‰µå¼•è½¦ï¼‰
                if option_type == "config":
                    # åŸºäºè§„èŒƒåŒ–æ–‡æœ¬åŒ…å«åŒ¹é…
                    from backend.app.services.search_service import SearchService
                    target = SearchService._norm_text(option_value)
                    if target:
                        next_filtered = []
                        for r in filtered_results:
                            d = r.diagram
                            blob = SearchService._diagram_blob(d)
                            if target in blob:
                                next_filtered.append(r)
                        filtered_results = next_filtered
                        conv_state.search_results = filtered_results
                
                # æ£€æŸ¥ç­›é€‰åçš„ç»“æœæ•°é‡
                if not filtered_results:
                    conv_state.update_state(ConversationStateEnum.COMPLETED)
                    conv_state.add_message("assistant", f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸ã€Œ{option_value}ã€ç›¸å…³çš„ç”µè·¯å›¾ã€‚è¯·å°è¯•å…¶ä»–é€‰é¡¹æˆ–é‡æ–°æœç´¢ã€‚")
                    return ChatResponse(
                        message=f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸ã€Œ{option_value}ã€ç›¸å…³çš„ç”µè·¯å›¾ã€‚è¯·å°è¯•å…¶ä»–é€‰é¡¹æˆ–é‡æ–°æœç´¢ã€‚",
                        session_id=session_id
                    )
                
                # å¦‚æœç­›é€‰åç»“æœâ‰¤5ä¸ªï¼Œç›´æ¥è¿”å›ç»“æœ
                max_results = request.max_results or 5
                if len(filtered_results) <= max_results:
                    formatted_results = []
                    for result in filtered_results:
                        formatted_results.append({
                            "id": result.diagram.id,
                            "file_name": result.diagram.file_name,
                            "hierarchy_path": " -> ".join(result.diagram.hierarchy_path),
                            "score": round(result.score, 2),
                            "brand": result.diagram.brand,
                            "model": result.diagram.model,
                            "diagram_type": result.diagram.diagram_type
                        })
                    
                    # è‹¥ç”¨æˆ·æŸ¥è¯¢åŒ…å«â€œç”µè·¯å›¾â€ï¼Œä¸”ç­›é€‰ååªå‰©å•ä¸€å›¾çº¸ç±»å‹ï¼Œåˆ™åŠ ä¸€æ®µç¡®è®¤è¯æœ¯ï¼ˆæ›´è´´è¿‘ä¸šåŠ¡æœŸæœ›ï¼‰
                    preface = ""
                    try:
                        q0 = (conv_state.current_query or "")
                        unique_types = {r.diagram.diagram_type for r in filtered_results if getattr(r.diagram, "diagram_type", None)}
                        if ("ç”µè·¯å›¾" in q0) and len(unique_types) == 1:
                            only_type = next(iter(unique_types))
                            preface = f"æ˜ç™½äº†ã€‚æŸ¥çœ‹åŒ…å«ç”µè·¯å›¾çš„æ•°æ®ï¼Œå‘ç°{pre_filter_total}æ¡æ•°æ®ä¸­å›¾çº¸ç±»å‹åªæœ‰â€œ{only_type}â€ï¼Œæˆ‘ç›´æ¥æŠŠç»“æœåˆ—å‡ºæ¥ï¼š\n\n"
                    except Exception:
                        preface = ""

                    message = preface + f"å·²ä¸ºæ‚¨æ‰¾åˆ°ä»¥ä¸‹ç”µè·¯å›¾ï¼š\n\n"
                    for i, result in enumerate(formatted_results, 1):
                        message += f"{i}. [ID: {result['id']}] {result['file_name']}\n"
                        message += f"   è·¯å¾„: {result['hierarchy_path']}\n"
                        if result['brand'] or result['model']:
                            attrs = []
                            if result['brand']:
                                attrs.append(f"å“ç‰Œ: {result['brand']}")
                            if result['model']:
                                attrs.append(f"å‹å·: {result['model']}")
                            if result['diagram_type']:
                                attrs.append(f"ç±»å‹: {result['diagram_type']}")
                            if attrs:
                                message += f"   {', '.join(attrs)}\n"
                        message += "\n"
                    
                    conv_state.update_state(ConversationStateEnum.COMPLETED)
                    conv_state.add_message("assistant", message)
                    
                    return ChatResponse(
                        message=message,
                        results=formatted_results,
                        needs_choice=False,
                        session_id=session_id
                    )
                
                # å¦‚æœç­›é€‰åç»“æœä»ç„¶>5ä¸ªï¼Œç»§ç»­ç”Ÿæˆé€‰æ‹©é¢˜
                # ç»§ç»­åˆ°ä¸‹é¢çš„é€»è¾‘å¤„ç†
                query = option_value  # ä½¿ç”¨é€‰é¡¹å€¼ä½œä¸ºæ–°çš„æŸ¥è¯¢
            else:
                conv_state.add_message("assistant", "æŠ±æ­‰ï¼Œæ— æ³•è¯†åˆ«æ‚¨çš„é€‰æ‹©ã€‚è¯·é‡æ–°é€‰æ‹©æˆ–è¾“å…¥é€‰é¡¹åç§°ã€‚")
                return ChatResponse(
                    message="æŠ±æ­‰ï¼Œæ— æ³•è¯†åˆ«æ‚¨çš„é€‰æ‹©ã€‚è¯·é‡æ–°é€‰æ‹©æˆ–è¾“å…¥é€‰é¡¹åç§°ã€‚",
                    session_id=session_id
                )
        else:
            # æ²¡æœ‰é€‰é¡¹æ•°æ®ï¼Œé‡æ–°æœç´¢
            pass
    
    # å¦‚æœç”¨æˆ·è¾“å…¥æ˜¯æ–‡æœ¬é€‰é¡¹åç§°ï¼Œä¹Ÿå°è¯•åŒ¹é…
    elif conv_state.state == ConversationStateEnum.NEEDS_CHOICE and conv_state.current_options:
        # å°è¯•åŒ¹é…é€‰é¡¹åç§°
        matched_option = None
        for option in conv_state.current_options:
            if query.lower() in option.get('name', '').lower() or \
               option.get('name', '').lower() in query.lower():
                matched_option = option
                break
        
        if matched_option:
            # å¤„ç†åŒ¹é…çš„é€‰é¡¹
            conv_state.add_filter(
                matched_option.get('type', 'unknown'),
                matched_option.get('name', '')
            )
            
            option_type = matched_option.get('type')
            option_value = matched_option.get('name')
            
            filtered_results = conv_state.search_results
            if option_type == "brand":
                filtered_results = search_service.filter_by_hierarchy(
                    filtered_results, brand=option_value
                )
            elif option_type == "model":
                filtered_results = search_service.filter_by_hierarchy(
                    filtered_results, model=option_value
                )
            elif option_type == "type":
                filtered_results = search_service.filter_by_hierarchy(
                    filtered_results, diagram_type=option_value
                )
            elif option_type == "variant":
                # è½¦å‹å˜ä½“ï¼šæŒ‰æ–‡ä»¶åå‰ç¼€ç²¾ç¡®åˆ†ç»„ï¼ˆä¾‹å¦‚ â€œä¸œé£å¤©é¾™KL_6x4ç¯å«è½¦â€ï¼‰
                base = (option_value or "").strip()
                for suf in (" ç³»åˆ—", "ç³»åˆ—"):
                    if base.endswith(suf):
                        base = base[: -len(suf)].strip()
                        break
                next_filtered = []
                for r in filtered_results:
                    k = variant_key_for_query(r.diagram.file_name or "", conv_state.current_query or "")
                    if k and k == base:
                        next_filtered.append(r)
                filtered_results = next_filtered
            elif option_type == "brand_model":
                # å“ç‰Œ+å‹å·ç»„åˆï¼šè§£æé€‰é¡¹å€¼ï¼ˆå¦‚"ä¸œé£ DOC"ã€"ä¸œé£ VEC"ç­‰ï¼‰
                brand, model = search_service._parse_brand_model(option_value)
                if brand and model:
                    # å…ˆæŒ‰å“ç‰Œç­›é€‰
                    filtered_results = search_service.filter_by_hierarchy(
                        filtered_results, brand=brand
                    )
                    # å†æŒ‰å‹å·ç­›é€‰ï¼ˆæ”¯æŒå±‚çº§è·¯å¾„åŒ¹é…ï¼‰
                    if filtered_results:
                        filtered_diagrams = HierarchyUtil.filter_by_model(
                            [r.diagram for r in filtered_results], model
                        )
                        filtered_ids = {d.id for d in filtered_diagrams}
                        filtered_results = [r for r in filtered_results if r.diagram.id in filtered_ids]
                elif brand:
                    filtered_results = search_service.filter_by_hierarchy(
                        filtered_results, brand=brand
                    )
            
            conv_state.search_results = filtered_results
            conv_state.current_options = []
            conv_state.option_type = None
            # æ”¯æŒé…ç½®/è½´å‹ç­›é€‰ï¼ˆå¦‚ 6x4 ç‰µå¼•è½¦ï¼‰
            if option_type == "config":
                from backend.app.services.search_service import SearchService
                target = SearchService._norm_text(option_value)
                if target:
                    next_filtered = []
                    for r in filtered_results:
                        d = r.diagram
                        blob = SearchService._diagram_blob(d)
                        if target in blob:
                            next_filtered.append(r)
                    filtered_results = next_filtered
                    conv_state.search_results = filtered_results
            
            # æ£€æŸ¥ç­›é€‰åçš„ç»“æœæ•°é‡
            if not filtered_results:
                conv_state.update_state(ConversationStateEnum.COMPLETED)
                conv_state.add_message("assistant", f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸ã€Œ{option_value}ã€ç›¸å…³çš„ç”µè·¯å›¾ã€‚è¯·å°è¯•å…¶ä»–é€‰é¡¹æˆ–é‡æ–°æœç´¢ã€‚")
                return ChatResponse(
                    message=f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸ã€Œ{option_value}ã€ç›¸å…³çš„ç”µè·¯å›¾ã€‚è¯·å°è¯•å…¶ä»–é€‰é¡¹æˆ–é‡æ–°æœç´¢ã€‚",
                    session_id=session_id
                )
            
            # å¦‚æœç­›é€‰åç»“æœâ‰¤5ä¸ªï¼Œç›´æ¥è¿”å›ç»“æœ
            max_results = request.max_results or 5
            if len(filtered_results) <= max_results:
                formatted_results = []
                for result in filtered_results:
                    formatted_results.append({
                        "id": result.diagram.id,
                        "file_name": result.diagram.file_name,
                        "hierarchy_path": " -> ".join(result.diagram.hierarchy_path),
                        "score": round(result.score, 2),
                        "brand": result.diagram.brand,
                        "model": result.diagram.model,
                        "diagram_type": result.diagram.diagram_type
                    })
                
                # è‹¥ç”¨æˆ·æŸ¥è¯¢åŒ…å«â€œç”µè·¯å›¾â€ï¼Œä¸”ç­›é€‰ååªå‰©å•ä¸€å›¾çº¸ç±»å‹ï¼Œåˆ™åŠ ä¸€æ®µç¡®è®¤è¯æœ¯ï¼ˆæ›´è´´è¿‘ä¸šåŠ¡æœŸæœ›ï¼‰
                preface = ""
                try:
                    q0 = (conv_state.current_query or "")
                    unique_types = {r.diagram.diagram_type for r in filtered_results if getattr(r.diagram, "diagram_type", None)}
                    if ("ç”µè·¯å›¾" in q0) and len(unique_types) == 1:
                        only_type = next(iter(unique_types))
                        # æ³¨æ„ï¼šè¿™é‡Œçš„ pre_filter_total åœ¨â€œæ–‡æœ¬å‘½ä¸­é€‰é¡¹â€åˆ†æ”¯ä¹Ÿåº”å–ç­›é€‰å‰çš„æ€»æ•°
                        preface = f"æ˜ç™½äº†ã€‚æŸ¥çœ‹åŒ…å«ç”µè·¯å›¾çš„æ•°æ®ï¼Œå‘ç°{len(conv_state.search_results or [])}æ¡æ•°æ®ä¸­å›¾çº¸ç±»å‹åªæœ‰â€œ{only_type}â€ï¼Œæˆ‘ç›´æ¥æŠŠç»“æœåˆ—å‡ºæ¥ï¼š\n\n"
                except Exception:
                    preface = ""

                message = preface + f"å·²ä¸ºæ‚¨æ‰¾åˆ°ä»¥ä¸‹ç”µè·¯å›¾ï¼š\n\n"
                for i, result in enumerate(formatted_results, 1):
                    message += f"{i}. [ID: {result['id']}] {result['file_name']}\n"
                    message += f"   è·¯å¾„: {result['hierarchy_path']}\n"
                    if result['brand'] or result['model']:
                        attrs = []
                        if result['brand']:
                            attrs.append(f"å“ç‰Œ: {result['brand']}")
                        if result['model']:
                            attrs.append(f"å‹å·: {result['model']}")
                        if result['diagram_type']:
                            attrs.append(f"ç±»å‹: {result['diagram_type']}")
                        if attrs:
                            message += f"   {', '.join(attrs)}\n"
                    message += "\n"
                
                conv_state.update_state(ConversationStateEnum.COMPLETED)
                conv_state.add_message("assistant", message)
                
                return ChatResponse(
                    message=message,
                    results=formatted_results,
                    needs_choice=False,
                    session_id=session_id
                )
            
            # å¦‚æœç­›é€‰åç»“æœä»ç„¶>5ä¸ªï¼Œç»§ç»­ç”Ÿæˆé€‰æ‹©é¢˜
            query = option_value
    
    # æ‰§è¡Œæ„å›¾ç†è§£
    intent_result = None
    try:
        intent_result = llm_service.parse_intent(query)
        conv_state.intent_result = intent_result
    except Exception as e:
        print(f"âš ï¸ æ„å›¾ç†è§£å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨å…³é”®è¯æœç´¢")
        # æ„å›¾ç†è§£å¤±è´¥æ—¶ï¼Œç»§ç»­ä½¿ç”¨å…³é”®è¯æœç´¢
    
    # æ›´æ–°å¯¹è¯çŠ¶æ€
    conv_state.update_state(ConversationStateEnum.SEARCHING)
    conv_state.current_query = query
    
    # è®°å½•ç”¨æˆ·å·²æŒ‡å®šçš„å“ç‰Œ/ç±»å‹ï¼Œç”¨äºåç»­è¿‡æ»¤å’Œé¿å…é‡å¤æé—®
    brand_already_specified = intent_result.has_brand() if intent_result else False
    type_already_specified = intent_result.has_diagram_type() if intent_result else False
    brand_tokens = []
    if brand_already_specified and intent_result.brand:
        brand_tokens.append(intent_result.brand)
        base_brand_hints = ["ä¸œé£", "è§£æ”¾", "é‡æ±½", "æ¬§æ›¼", "ä¹˜é¾™", "æ°ç‹®", "è±ªç€š", "è±ªæ±‰", "å¤§é€š"]
        for hint in base_brand_hints:
            if hint in intent_result.brand:
                brand_tokens.append(hint)

    # æ‰§è¡Œæœç´¢
    logic = request.logic or "AND"
    max_results = request.max_results or 5
    max_options = max_results  # ç”¨äºé™åˆ¶é€‰æ‹©é¢˜é€‰é¡¹æ•°é‡
    
    # ä½¿ç”¨æ„å›¾ç†è§£ç»“æœè¿›è¡Œæœç´¢
    if intent_result:
        scored_results = search_service.search_with_intent(
            intent_result=intent_result,
            logic=logic,
            max_results=1000,  # è·å–è¶³å¤Ÿå¤šçš„ç»“æœç”¨äºåˆ†æ
            use_fuzzy=True
        )
    else:
        # é™çº§ä¸ºå…³é”®è¯æœç´¢
        scored_results = search_service.search(
            query=query,
            logic=logic,
            max_results=1000,
            use_fuzzy=True
        )
    
    # å¦‚æœANDé€»è¾‘æ— ç»“æœï¼Œå°è¯•ORé€»è¾‘
    # é‡è¦ï¼šå½“ç”¨æˆ·è¾“å…¥ä¸­åŒ…å«å¤šä¸ªæ ¸å¿ƒå…³é”®è¯æ—¶ï¼Œä¸åº”è‡ªåŠ¨é™çº§ä¸ºORï¼ˆä¼šå¯¼è‡´â€œåªå‘½ä¸­éƒ¨åˆ†å…³é”®è¯â€çš„ç»“æœæ··å…¥ï¼‰
    # ä»…å½“â€œæ ¸å¿ƒå…³é”®è¯<=1â€ï¼ˆä¾‹å¦‚åªè¾“å…¥ä¸€ä¸ªè¯ï¼‰æ—¶ï¼Œæ‰å…è®¸AND->ORçš„å…œåº•ã€‚
    if not scored_results and logic.upper() == "AND":
        extracted_keywords = search_service._extract_keywords(query)
        core_kw_count = len([k for k in extracted_keywords if k and len(k.strip()) > 0])
        allow_or_fallback = core_kw_count <= 1

        if not allow_or_fallback:
            conv_state.update_state(ConversationStateEnum.COMPLETED)
            error_message = f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°**åŒæ—¶åŒ¹é…**æ‚¨å…³é”®è¯çš„ç»“æœï¼ˆANDï¼‰ã€‚\n\nå»ºè®®ï¼š\n- æ£€æŸ¥å…³é”®è¯æ˜¯å¦è¿‡äºå…·ä½“ï¼ˆå¦‚é’ˆè„šå›¾/ç‰ˆæœ¬å·ï¼‰\n- å°è¯•è¡¥å……æˆ–æ›¿æ¢å…³é”®è¯ï¼ˆä¾‹å¦‚ï¼šä»ªè¡¨å›¾/ä»ªè¡¨ç”µè·¯å›¾ï¼‰\n- æˆ–è€…å‡å°‘ä¸€ä¸ªé™å®šè¯å†è¯•"
            conv_state.add_message("assistant", error_message)
            return ChatResponse(
                message=error_message,
                session_id=session_id
            )

        if intent_result:
            scored_results = search_service.search_with_intent(
                intent_result=intent_result,
                logic="OR",
                max_results=1000,
                use_fuzzy=True
            )
        else:
            scored_results = search_service.search(
                query=query,
                logic="OR",
                max_results=1000,
                use_fuzzy=True
            )
    
    # å»é‡
    scored_results = search_service.deduplicate_results(scored_results)

    # å¦‚æœç”¨æˆ·å·²ç»æ˜ç¡®å“ç‰Œ/ç±»å‹ï¼Œå…ˆè¿›è¡Œå¼ºè¿‡æ»¤ï¼Œé¿å…å‡ºç°æ— å…³é€‰é¡¹
    if intent_result and (brand_already_specified or type_already_specified):
        filtered_results = search_service.filter_by_hierarchy(
            scored_results,
            brand=intent_result.brand if brand_already_specified else None,
            diagram_type=intent_result.diagram_type if type_already_specified else None
        )
        if filtered_results:
            scored_results = filtered_results
        else:
            conv_state.update_state(ConversationStateEnum.COMPLETED)
            error_message = f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°åŒæ—¶åŒ¹é…ã€Œ{intent_result.brand or ''}ã€å’Œã€Œ{intent_result.diagram_type or ''}ã€çš„ç”µè·¯å›¾ã€‚è¯·ç¡®è®¤å…³é”®è¯æˆ–æä¾›æ›´å¤šä¿¡æ¯ã€‚"
            conv_state.add_message("assistant", error_message)
            return ChatResponse(
                message=error_message,
                session_id=session_id
            )
    
    # æ›´æ–°å¯¹è¯çŠ¶æ€ä¸­çš„æœç´¢ç»“æœ
    conv_state.search_results = scored_results
    
    if not scored_results:
        conv_state.update_state(ConversationStateEnum.COMPLETED)
        error_message = f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸ã€Œ{query}ã€ç›¸å…³çš„ç”µè·¯å›¾ã€‚\n\nå»ºè®®ï¼š\n1. å°è¯•ä½¿ç”¨å…¶ä»–å…³é”®è¯\n2. æ£€æŸ¥æ‹¼å†™æ˜¯å¦æ­£ç¡®\n3. å°è¯•ä½¿ç”¨æ›´é€šç”¨çš„å…³é”®è¯ï¼ˆå¦‚å“ç‰Œåç§°ï¼‰"
        conv_state.add_message("assistant", error_message)
        return ChatResponse(
            message=error_message,
            session_id=session_id
        )
    
    total_found = len(scored_results)
    
    print(f"ğŸ” æœç´¢ç»“æœ: {total_found} ä¸ªï¼Œmax_results: {max_results}")
    
    # å¦‚æœç»“æœè¶…è¿‡5ä¸ªï¼Œå°è¯•ç”Ÿæˆé€‰æ‹©é¢˜å¼•å¯¼ç”¨æˆ·ç¼©å°èŒƒå›´
    # é‡è¦ï¼šå½“ç»“æœ>5ä¸ªæ—¶ï¼Œå¿…é¡»ç”Ÿæˆé€‰æ‹©é¢˜ï¼Œä¸èƒ½ç›´æ¥è¿”å›ç»“æœ
    if total_found > max_results:
        print(f"âœ… ç»“æœæ•°({total_found}) > max_results({max_results})ï¼Œè¿›å…¥é€‰æ‹©é¢˜ç”Ÿæˆé€»è¾‘")
        
        # å¦‚æœæ„å›¾ç†è§£è¯†åˆ«åˆ°äº†å“ç‰Œå’Œç±»å‹ï¼Œå°†å®ƒä»¬æ·»åŠ åˆ°ç­›é€‰å†å²ï¼ˆç”¨äºæŒ‡å¯¼é€‰æ‹©é¢˜ç”Ÿæˆï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œä¸å®é™…ç­›é€‰ç»“æœï¼Œåªæ˜¯è®°å½•ç”¨æˆ·æ„å›¾ï¼Œä»¥ä¾¿ç”Ÿæˆåˆé€‚çš„é€‰æ‹©é¢˜
        temp_filter_history = list(conv_state.filter_history)  # å¤åˆ¶ä¸€ä»½ï¼Œé¿å…ä¿®æ”¹åŸå§‹å†å²
        if intent_result:
            # å¦‚æœè¯†åˆ«åˆ°äº†å“ç‰Œï¼Œæ·»åŠ åˆ°ä¸´æ—¶ç­›é€‰å†å²
            if intent_result.has_brand() and not any(f.get('type') == 'brand' for f in temp_filter_history):
                temp_filter_history.append({
                    "type": "brand",
                    "value": intent_result.brand
                })
            # å¦‚æœè¯†åˆ«åˆ°äº†ç±»å‹ï¼Œæ·»åŠ åˆ°ä¸´æ—¶ç­›é€‰å†å²
            if intent_result.has_diagram_type() and not any(f.get('type') == 'type' for f in temp_filter_history):
                temp_filter_history.append({
                    "type": "type",
                    "value": intent_result.diagram_type
                })
        
        # è·å–å·²ç­›é€‰çš„ç±»å‹ï¼ˆé¿å…é‡å¤æé—®ï¼‰
        excluded_types = [f.get('type') for f in temp_filter_history]
        
        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆä½¿ç”¨ä¸´æ—¶ç­›é€‰å†å²ï¼‰
        context = {
            "filter_history": temp_filter_history,
            "current_query": conv_state.current_query,
            "total_results": total_found,
            "intent_result": {
                "brand": intent_result.brand if intent_result else None,
                "model": intent_result.model if intent_result else None,
                "diagram_type": intent_result.diagram_type if intent_result else None
            } if intent_result else None
        }
        
        # ç”Ÿæˆé€‰æ‹©é¢˜ï¼ˆä½¿ç”¨LLMç”Ÿæˆè‡ªç„¶çš„é—®é¢˜æ–‡æœ¬ï¼‰
        question_data = question_service.generate_question(
            scored_results,
            min_options=2,
            max_options=max_options,
            excluded_types=excluded_types if excluded_types else None,
            context=context,
            use_llm=True
        )
        
        print(f"ğŸ” question_data: {question_data is not None}")
        
        if question_data:
            print(f"âœ… æˆåŠŸç”Ÿæˆé€‰æ‹©é¢˜ï¼Œé€‰é¡¹æ•°: {len(question_data.get('options', []))}")
            # æ›´æ–°å¯¹è¯çŠ¶æ€
            conv_state.update_state(ConversationStateEnum.NEEDS_CHOICE)
            conv_state.current_options = question_data['options']
            conv_state.option_type = question_data['option_type']
            
            # æ ¼å¼åŒ–æ¶ˆæ¯
            message = question_service.format_question_message(question_data)
            
            conv_state.add_message("assistant", message)
            
            return ChatResponse(
                message=message,
                results=None,
                options=question_data['options'],
                needs_choice=True,
                session_id=session_id
            )
        else:
            # æ— æ³•ç”Ÿæˆé€‰æ‹©é¢˜ï¼Œå°è¯•ä»å±‚çº§è·¯å¾„ä¸­æå–æ›´ç»†ç²’åº¦çš„é€‰é¡¹
            # å°è¯•æå–å±‚çº§è·¯å¾„ä¸­çš„ä¸åŒå±‚çº§ä½œä¸ºé€‰é¡¹
            print(f"âš ï¸ generate_questionè¿”å›Noneï¼Œå°è¯•fallbacké€»è¾‘ï¼Œç»“æœæ•°: {total_found}")
            
            # å°è¯•æå–ä¸åŒå±‚çº§çš„é€‰é¡¹
            all_levels = HierarchyUtil.get_all_levels([r.diagram for r in scored_results])
            
            # å°è¯•æ‰¾åˆ°æœ‰å¤šä¸ªé€‰é¡¹çš„å±‚çº§
            best_option_type = None
            best_options = []
            
            # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥ï¼šå“ç‰Œ+å‹å·ç»„åˆ -> å“ç‰Œ -> å‹å· -> ç±»å‹ -> ç±»åˆ«
            # ä¼˜å…ˆå°è¯•ä»å±‚çº§è·¯å¾„ä¸­æå–å“ç‰Œ+å‹å·ç»„åˆ
            try:
                brand_model_options = question_service._extract_options_from_hierarchy(
                    scored_results, max_options
                )
                print(f"âš ï¸ ä»å±‚çº§è·¯å¾„æå–å“ç‰Œ+å‹å·ç»„åˆ: {len(brand_model_options) if brand_model_options else 0} ä¸ªé€‰é¡¹")
                if brand_model_options and len(brand_model_options) >= 2:
                    best_option_type = "brand_model"
                    best_options = brand_model_options
            except Exception as e:
                print(f"âš ï¸ æå–å“ç‰Œ+å‹å·ç»„åˆå¤±è´¥: {str(e)}")
            
            # å¦‚æœå“ç‰Œ+å‹å·ç»„åˆå¤±è´¥ï¼Œå°è¯•å…¶ä»–ç±»å‹
            if not best_options:
                print(f"âš ï¸ å°è¯•å…¶ä»–ç±»å‹é€‰é¡¹ï¼Œå·²æ’é™¤ç±»å‹: {excluded_types}")
                for opt_type, level_set in [("brand", all_levels.get("brands", set())),
                                            ("model", all_levels.get("models", set())),
                                            ("type", all_levels.get("types", set())),
                                            ("category", all_levels.get("categories", set()))]:
                    if opt_type not in (excluded_types or []):
                        print(f"âš ï¸ æ£€æŸ¥ç±»å‹ {opt_type}ï¼Œé€‰é¡¹æ•°: {len(level_set)}")
                        if len(level_set) >= 2:
                            # è½¬æ¢ä¸ºé€‰é¡¹æ ¼å¼
                            options = [{"name": name, "count": sum(1 for r in scored_results 
                                                                   if (opt_type == "brand" and r.diagram.brand == name) or
                                                                      (opt_type == "model" and r.diagram.model == name) or
                                                                      (opt_type == "type" and r.diagram.diagram_type == name) or
                                                                      (opt_type == "category" and r.diagram.vehicle_category == name))}
                                      for name in list(level_set)[:max_options]]
                            options.sort(key=lambda x: x["count"], reverse=True)
                            print(f"âš ï¸ ç±»å‹ {opt_type} ç”Ÿæˆé€‰é¡¹æ•°: {len(options)}")
                            if len(options) >= 2:
                                best_option_type = opt_type
                                best_options = options[:max_options]
                                break
            
            if best_option_type and best_options:
                # ç”Ÿæˆé—®é¢˜ï¼ˆä½¿ç”¨LLMæˆ–é»˜è®¤æ¨¡æ¿ï¼‰
                try:
                    # ä½¿ç”¨å·²ç»åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥çš„ llm_service
                    question_text = llm_service.generate_question_text(
                        option_type=best_option_type,
                        options=best_options,
                        total_count=total_found,
                        context=context
                    )
                except Exception as e:
                    print(f"âš ï¸ LLMç”Ÿæˆé—®é¢˜å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿")
                    question_text = question_service._generate_question_text(
                        best_option_type, total_found, context
                    )
                
                option_labels = ['A', 'B', 'C', 'D', 'E']
                formatted_options = []
                for i, option in enumerate(best_options[:max_options]):
                    formatted_options.append({
                        "label": option_labels[i],
                        "name": option['name'],
                        "count": option['count'],
                        "type": best_option_type
                    })
                
                question_data = {
                    "question": question_text,
                    "options": formatted_options,
                    "option_type": best_option_type
                }
                
                # æ›´æ–°å¯¹è¯çŠ¶æ€
                conv_state.update_state(ConversationStateEnum.NEEDS_CHOICE)
                conv_state.current_options = formatted_options
                conv_state.option_type = best_option_type
                
                # æ ¼å¼åŒ–æ¶ˆæ¯
                message = question_service.format_question_message(question_data)
                
                conv_state.add_message("assistant", message)
                
                return ChatResponse(
                    message=message,
                    results=None,
                    options=formatted_options,
                    needs_choice=True,
                    session_id=session_id
                )
            
            # å¦‚æœä»ç„¶æ— æ³•ç”Ÿæˆé€‰æ‹©é¢˜ï¼Œå¼ºåˆ¶å°è¯•ä»å±‚çº§è·¯å¾„ä¸­æå–é€‰é¡¹
            # è¿™æ˜¯æœ€åçš„fallbackï¼Œå¿…é¡»ç”Ÿæˆé€‰æ‹©é¢˜
            if not best_options:
                print(f"âš ï¸ å°è¯•æœ€åçš„fallbackï¼šä»å±‚çº§è·¯å¾„ä¸­æå–ä»»æ„æœ‰åŒºåˆ†åº¦çš„é€‰é¡¹")
                try:
                    # å°è¯•ä»å±‚çº§è·¯å¾„ä¸­æå–ä»»æ„æœ‰åŒºåˆ†åº¦çš„é€‰é¡¹
                    hierarchy_options = {}
                    for result in scored_results:
                        diagram = result.diagram
                        if diagram.hierarchy_path and len(diagram.hierarchy_path) > 2:
                            # å°è¯•æå–å“ç‰Œåé¢çš„å±‚çº§
                            brand_pos = -1
                            if diagram.brand:
                                for i, level in enumerate(diagram.hierarchy_path):
                                    if diagram.brand in level or level == diagram.brand:
                                        brand_pos = i
                                        break
                            
                            if brand_pos != -1 and brand_pos + 1 < len(diagram.hierarchy_path):
                                level_value = diagram.hierarchy_path[brand_pos + 1]
                                level_value_clean = level_value.replace('*', '').strip()
                                if level_value_clean and level_value_clean != diagram.brand:
                                    option_name = f"{diagram.brand} {level_value_clean}"
                                    hierarchy_options[option_name] = hierarchy_options.get(option_name, 0) + 1
                            else:
                                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å“ç‰Œï¼Œå°è¯•æå–å±‚çº§è·¯å¾„ä¸­çš„å…¶ä»–å±‚çº§
                                for i, level in enumerate(diagram.hierarchy_path):
                                    if i > 0 and level and level != "ç”µè·¯å›¾" and len(level) > 1:
                                        # è·³è¿‡ç¬¬ä¸€ä¸ªå±‚çº§ï¼ˆé€šå¸¸æ˜¯"ç”µè·¯å›¾"ï¼‰
                                        hierarchy_options[level] = hierarchy_options.get(level, 0) + 1
                                        break
                    
                    print(f"âš ï¸ ä»å±‚çº§è·¯å¾„æå–åˆ° {len(hierarchy_options)} ä¸ªé€‰é¡¹")
                    
                    if len(hierarchy_options) >= 2:
                        # è½¬æ¢ä¸ºé€‰é¡¹æ ¼å¼
                        options = [
                            {"name": name, "count": count}
                            for name, count in sorted(hierarchy_options.items(), key=lambda x: x[1], reverse=True)[:max_options]
                        ]
                        
                        question_text = question_service._generate_question_text(
                            "brand_model", total_found, context
                        )
                        
                        option_labels = ['A', 'B', 'C', 'D', 'E']
                        formatted_options = []
                        for i, option in enumerate(options):
                            formatted_options.append({
                                "label": option_labels[i],
                                "name": option['name'],
                                "count": option['count'],
                                "type": "brand_model"
                            })
                        
                        question_data = {
                            "question": question_text,
                            "options": formatted_options,
                            "option_type": "brand_model"
                        }
                        
                        # æ›´æ–°å¯¹è¯çŠ¶æ€
                        conv_state.update_state(ConversationStateEnum.NEEDS_CHOICE)
                        conv_state.current_options = formatted_options
                        conv_state.option_type = "brand_model"
                        
                        # æ ¼å¼åŒ–æ¶ˆæ¯
                        message = question_service.format_question_message(question_data)
                        
                        conv_state.add_message("assistant", message)
                        
                        return ChatResponse(
                            message=message,
                            results=None,
                            options=formatted_options,
                            needs_choice=True,
                            session_id=session_id
                        )
                except Exception as e:
                    print(f"âš ï¸ Fallbacké€‰é¡¹ç”Ÿæˆå¤±è´¥: {str(e)}")
            
            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œå¼ºåˆ¶ç”Ÿæˆé€‰æ‹©é¢˜ï¼ˆå³ä½¿é€‰é¡¹ä¸å¤Ÿç†æƒ³ï¼‰
            if not best_options:
                print(f"âš ï¸ æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œå¼ºåˆ¶ç”Ÿæˆé€‰æ‹©é¢˜")
                # å¼ºåˆ¶ä»å±‚çº§è·¯å¾„ä¸­æå–é€‰é¡¹ï¼Œå³ä½¿åªæœ‰éƒ¨åˆ†åŒºåˆ†åº¦
                try:
                    hierarchy_options = {}
                    for result in scored_results:
                        diagram = result.diagram
                        if diagram.hierarchy_path:
                            # å°è¯•æå–å±‚çº§è·¯å¾„ä¸­çš„ä¸åŒå±‚çº§ä½œä¸ºé€‰é¡¹
                            for i, level in enumerate(diagram.hierarchy_path):
                                if i > 0 and level and level != "ç”µè·¯å›¾" and len(level.strip()) > 1:
                                    # è·³è¿‡ç¬¬ä¸€ä¸ªå±‚çº§ï¼ˆé€šå¸¸æ˜¯"ç”µè·¯å›¾"ï¼‰
                                    level_clean = level.replace('*', '').strip()
                                    if not level_clean:
                                        continue
                                    # å·²çŸ¥ç±»å‹æ—¶è·³è¿‡ç±»å‹ç›¸å…³å±‚çº§ï¼Œé¿å…å†æ¬¡è¯¢é—®ç±»å‹
                                    type_keywords = ['ç”µè·¯å›¾', 'ä»ªè¡¨', 'ECU', 'æ•´è½¦', 'çº¿è·¯', 'é’ˆè„š', 'æ¨¡å—', 'æ¥çº¿']
                                    if type_already_specified and any(k in level_clean for k in type_keywords):
                                        continue
                                    # å·²çŸ¥å“ç‰Œæ—¶è·³è¿‡å“ç‰Œå±‚çº§ï¼Œé¿å…æŠŠå“ç‰Œå½“é€‰é¡¹
                                    if brand_tokens and any(bt and (bt in level_clean or level_clean in bt) for bt in brand_tokens):
                                        continue
                                    if level_clean:
                                        hierarchy_options[level_clean] = hierarchy_options.get(level_clean, 0) + 1
                    
                    # å¦‚æœæå–åˆ°é€‰é¡¹ï¼Œä½¿ç”¨å®ƒä»¬
                    if len(hierarchy_options) >= 2 and not type_already_specified:
                        options = [
                            {"name": name, "count": count}
                            for name, count in sorted(hierarchy_options.items(), key=lambda x: x[1], reverse=True)[:max_options]
                        ]
                        
                        question_text = question_service._generate_question_text(
                            "type", total_found, context
                        )
                        
                        option_labels = ['A', 'B', 'C', 'D', 'E']
                        formatted_options = []
                        for i, option in enumerate(options):
                            formatted_options.append({
                                "label": option_labels[i],
                                "name": option['name'],
                                "count": option['count'],
                                "type": "type"
                            })
                        
                        question_data = {
                            "question": question_text,
                            "options": formatted_options,
                            "option_type": "type"
                        }
                        
                        conv_state.update_state(ConversationStateEnum.NEEDS_CHOICE)
                        conv_state.current_options = formatted_options
                        conv_state.option_type = "type"
                        
                        message = question_service.format_question_message(question_data)
                        conv_state.add_message("assistant", message)
                        
                        return ChatResponse(
                            message=message,
                            results=None,
                            options=formatted_options,
                            needs_choice=True,
                            session_id=session_id
                        )
                    elif len(hierarchy_options) >= 2:
                        # å¦‚æœç±»å‹å·²çŸ¥ï¼Œåˆ™å°†å±‚çº§é€‰é¡¹è§†ä¸ºç³»åˆ—/å‹å·é€‰é¡¹ç»§ç»­è¿½é—®
                        options = [
                            {"name": name, "count": count}
                            for name, count in sorted(hierarchy_options.items(), key=lambda x: x[1], reverse=True)[:max_options]
                        ]

                        question_text = question_service._generate_question_text(
                            "brand_model", total_found, context
                        )

                        option_labels = ['A', 'B', 'C', 'D', 'E']
                        formatted_options = []
                        for i, option in enumerate(options):
                            formatted_options.append({
                                "label": option_labels[i],
                                "name": option['name'],
                                "count": option['count'],
                                "type": "brand_model"
                            })

                        question_data = {
                            "question": question_text,
                            "options": formatted_options,
                            "option_type": "brand_model"
                        }

                        conv_state.update_state(ConversationStateEnum.NEEDS_CHOICE)
                        conv_state.current_options = formatted_options
                        conv_state.option_type = "brand_model"

                        message = question_service.format_question_message(question_data)
                        conv_state.add_message("assistant", message)

                        return ChatResponse(
                            message=message,
                            results=None,
                            options=formatted_options,
                            needs_choice=True,
                            session_id=session_id
                        )
                    else:
                        # å¦‚æœè¿å±‚çº§è·¯å¾„éƒ½æå–ä¸åˆ°è¶³å¤Ÿçš„é€‰é¡¹ï¼Œè‡³å°‘åŸºäºæ–‡ä»¶åç”Ÿæˆé€‰é¡¹
                        print(f"âš ï¸ å±‚çº§è·¯å¾„æå–å¤±è´¥ï¼Œå°è¯•åŸºäºæ–‡ä»¶åç”Ÿæˆé€‰é¡¹")
                        file_name_options = {}
                        for result in scored_results[:max_results * 2]:  # æ£€æŸ¥æ›´å¤šç»“æœä»¥æ‰¾åˆ°åŒºåˆ†åº¦
                            diagram = result.diagram
                            # ä»æ–‡ä»¶åä¸­æå–å…³é”®è¯ï¼ˆå»é™¤å“ç‰Œå’Œå¸¸è§è¯ï¼‰
                            file_name = diagram.file_name
                            # å°è¯•æå–æ–‡ä»¶åä¸­çš„å…³é”®éƒ¨åˆ†
                            if diagram.brand and diagram.brand in file_name:
                                # æå–å“ç‰Œåé¢çš„éƒ¨åˆ†
                                parts = file_name.split(diagram.brand, 1)
                                if len(parts) > 1:
                                    key_part = parts[1].split('.')[0].strip('_-. ')[:20]  # å–å‰20ä¸ªå­—ç¬¦
                                    if key_part and len(key_part) > 1:
                                        file_name_options[key_part] = file_name_options.get(key_part, 0) + 1
                            
                            # æˆ–è€…ç›´æ¥ä½¿ç”¨æ–‡ä»¶åçš„ä¸€éƒ¨åˆ†
                            if not file_name_options:
                                # æå–æ–‡ä»¶åä¸­çš„å…³é”®è¯ï¼ˆå»é™¤æ‰©å±•åï¼‰
                                name_part = file_name.split('.')[0]
                                if len(name_part) > 5:
                                    # å–æ–‡ä»¶åçš„ä¸€éƒ¨åˆ†ä½œä¸ºé€‰é¡¹
                                    key_part = name_part[:15]
                                    file_name_options[key_part] = file_name_options.get(key_part, 0) + 1
                        
                        if len(file_name_options) >= 2:
                            options = [
                                {"name": name, "count": count}
                                for name, count in sorted(file_name_options.items(), key=lambda x: x[1], reverse=True)[:max_options]
                            ]
                            
                            question_text = f"æ‰¾åˆ°äº† {total_found} ä¸ªç›¸å…³ç»“æœã€‚è¯·é€‰æ‹©æ‚¨éœ€è¦çš„ç±»å‹ï¼š"
                            
                            option_labels = ['A', 'B', 'C', 'D', 'E']
                            formatted_options = []
                            for i, option in enumerate(options):
                                formatted_options.append({
                                    "label": option_labels[i],
                                    "name": option['name'],
                                    "count": option['count'],
                                    "type": "type"
                                })
                            
                            question_data = {
                                "question": question_text,
                                "options": formatted_options,
                                "option_type": "type"
                            }
                            
                            conv_state.update_state(ConversationStateEnum.NEEDS_CHOICE)
                            conv_state.current_options = formatted_options
                            conv_state.option_type = "type"
                            
                            message = question_service.format_question_message(question_data)
                            conv_state.add_message("assistant", message)
                            
                            return ChatResponse(
                                message=message,
                                results=None,
                                options=formatted_options,
                                needs_choice=True,
                                session_id=session_id
                            )
                except Exception as e:
                    print(f"âš ï¸ å¼ºåˆ¶ç”Ÿæˆé€‰æ‹©é¢˜å¤±è´¥: {str(e)}")
                
                # å¦‚æœæ‰€æœ‰å¼ºåˆ¶ç”Ÿæˆæ–¹æ³•éƒ½å¤±è´¥ï¼Œè‡³å°‘ç”Ÿæˆä¸€ä¸ªåŸºäºç»“æœæ•°é‡çš„é€‰æ‹©é¢˜
                print(f"âš ï¸ æ‰€æœ‰å¼ºåˆ¶ç”Ÿæˆæ–¹æ³•éƒ½å¤±è´¥ï¼Œç”ŸæˆåŸºäºç»“æœçš„åˆ†ç»„é€‰æ‹©é¢˜")
                # å°†ç»“æœåˆ†æˆå‡ ç»„ï¼Œè®©ç”¨æˆ·é€‰æ‹©
                group_size = max(2, total_found // max_results)
                groups = []
                for i in range(0, min(total_found, max_results * 2), group_size):
                    group_results = scored_results[i:i+group_size]
                    if group_results:
                        # æå–è¿™ç»„ç»“æœçš„å…³é”®ç‰¹å¾
                        group_name = f"ç¬¬{i+1}-{min(i+group_size, total_found)}ä¸ªç»“æœ"
                        if group_results[0].diagram.brand:
                            group_name = f"{group_results[0].diagram.brand}ç›¸å…³"
                        groups.append({
                            "name": group_name,
                            "count": len(group_results),
                            "results": group_results
                        })
                
                if len(groups) >= 2:
                    question_text = f"æ‰¾åˆ°äº† {total_found} ä¸ªç›¸å…³ç»“æœã€‚è¯·é€‰æ‹©æ‚¨éœ€è¦çš„èŒƒå›´ï¼š"
                    
                    option_labels = ['A', 'B', 'C', 'D', 'E']
                    formatted_options = []
                    for i, group in enumerate(groups[:max_options]):
                        formatted_options.append({
                            "label": option_labels[i],
                            "name": group['name'],
                            "count": group['count'],
                            "type": "group"
                        })
                    
                    question_data = {
                        "question": question_text,
                        "options": formatted_options,
                        "option_type": "group"
                    }
                    
                    conv_state.update_state(ConversationStateEnum.NEEDS_CHOICE)
                    conv_state.current_options = formatted_options
                    conv_state.option_type = "group"
                    # ä¿å­˜åˆ†ç»„ç»“æœä»¥ä¾¿åç»­ä½¿ç”¨
                    conv_state.grouped_results = groups
                    
                    message = question_service.format_question_message(question_data)
                    conv_state.add_message("assistant", message)
                    
                    return ChatResponse(
                        message=message,
                        results=None,
                        options=formatted_options,
                        needs_choice=True,
                        session_id=session_id
                    )
                
                # å¦‚æœè¿åˆ†ç»„éƒ½å¤±è´¥ï¼Œè¿”å›é”™è¯¯æç¤ºï¼ˆè¿™ç§æƒ…å†µåº”è¯¥å¾ˆå°‘è§ï¼‰
                error_message = f"æ‰¾åˆ°äº† {total_found} ä¸ªç›¸å…³ç»“æœï¼Œä½†æ— æ³•ç”Ÿæˆé€‰æ‹©é¢˜ã€‚è¯·å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯é‡æ–°æœç´¢ã€‚"
                conv_state.update_state(ConversationStateEnum.COMPLETED)
                conv_state.add_message("assistant", error_message)
                return ChatResponse(
                    message=error_message,
                    session_id=session_id
                )
    else:
        # ç»“æœâ‰¤5ä¸ªï¼Œç›´æ¥è¿”å›æ‰€æœ‰ç»“æœ
        print(f"âœ… ç»“æœæ•°({total_found}) <= max_results({max_results})ï¼Œç›´æ¥è¿”å›ç»“æœ")
        formatted_results = []
        for result in scored_results[:max_results]:
            formatted_results.append({
                "id": result.diagram.id,
                "file_name": result.diagram.file_name,
                "hierarchy_path": " -> ".join(result.diagram.hierarchy_path),
                "score": round(result.score, 2),
                "brand": result.diagram.brand,
                "model": result.diagram.model,
                "diagram_type": result.diagram.diagram_type
            })
        
        message = f"æ‰¾åˆ°äº† {total_found} ä¸ªç›¸å…³ç»“æœï¼š\n\n"
        for i, result in enumerate(formatted_results, 1):
            message += f"{i}. [ID: {result['id']}] {result['file_name']}\n"
            message += f"   è·¯å¾„: {result['hierarchy_path']}\n"
            if result['brand'] or result['model']:
                attrs = []
                if result['brand']:
                    attrs.append(f"å“ç‰Œ: {result['brand']}")
                if result['model']:
                    attrs.append(f"å‹å·: {result['model']}")
                if result['diagram_type']:
                    attrs.append(f"ç±»å‹: {result['diagram_type']}")
                if attrs:
                    message += f"   {', '.join(attrs)}\n"
            message += "\n"
        
        conv_state.update_state(ConversationStateEnum.COMPLETED)
        conv_state.add_message("assistant", message)
        
        return ChatResponse(
            message=message,
            results=formatted_results,
            needs_choice=False,
            session_id=session_id
        )
