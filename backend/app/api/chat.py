"""
èŠå¤©API
é›†æˆæ„å›¾ç†è§£å’Œå¯¹è¯ç®¡ç†
"""
from fastapi import APIRouter
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import re
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from backend.app.services.search_service import get_search_service
from backend.app.services.llm_service import get_llm_service
from backend.app.services.question_service import get_question_service
from backend.app.models.conversation import (
    get_conversation_manager,
    ConversationStateEnum
)
from backend.app.utils.hierarchy_util import HierarchyUtil
from backend.app.utils.variant_util import variant_key_for_query

router = APIRouter()

def _filter_conditions_text(filter_history: List[Dict[str, Any]]) -> str:
    """
    å°† filter_history å½’ä¸€æˆå¯è¯»çš„â€œå½“å‰ç­›é€‰æ¡ä»¶â€æ–‡æœ¬ã€‚
    è§„åˆ™ï¼šåŒä¸€ type åªä¿ç•™æœ€åä¸€æ¬¡çš„ valueï¼Œå¹¶æŒ‰å›ºå®šé¡ºåºè¾“å‡ºã€‚
    """
    label_map = {
        "brand": "å“ç‰Œ",
        "model": "å‹å·/ç³»åˆ—",
        "type": "å›¾çº¸ç±»å‹",
        "variant": "è½¦å‹å˜ä½“",
        "brand_model": "å“ç‰Œ+ç³»åˆ—",
        "config": "é…ç½®/ç”¨é€”",
        "result": "èµ„æ–™",
    }
    last_by_type: Dict[str, str] = {}
    for f in (filter_history or []):
        try:
            t = str(f.get("type") or "").strip()
            v = str(f.get("value") or "").strip()
        except Exception:
            continue
        if t and v:
            last_by_type[t] = v
    if not last_by_type:
        return ""
    # è‹¥ brand + model éƒ½å·²å­˜åœ¨ï¼Œåˆ™ä¸å†é¢å¤–å±•ç¤º brand_modelï¼ˆé¿å…é‡å¤/å™ªéŸ³ï¼‰
    if "brand" in last_by_type and "model" in last_by_type and "brand_model" in last_by_type:
        last_by_type.pop("brand_model", None)

    order = ["brand", "model", "type", "variant", "config", "result", "brand_model"]
    parts: List[str] = []
    for t in order:
        if t in last_by_type:
            parts.append(f"{label_map.get(t, t)}={last_by_type[t]}")
    for t, v in last_by_type.items():
        if t not in order:
            parts.append(f"{label_map.get(t, t)}={v}")
    return "ï¼Œ".join(parts)


def _build_selection_summary(
    option_value: str,
    pre_total: int,
    post_total: int,
    filter_history: List[Dict[str, Any]],
) -> str:
    ov = (option_value or "").strip()
    cond_txt = _filter_conditions_text(filter_history)
    if cond_txt:
        return (
            f"æ˜ç™½äº†ï¼Œå·²æ ¹æ®æ‚¨é€‰æ‹©çš„â€œ{ov}â€è¿›ä¸€æ­¥ç­›é€‰ï¼š{pre_total} â†’ {post_total} æ¡ã€‚\n"
            f"å½“å‰ç­›é€‰æ¡ä»¶ï¼š{cond_txt}"
        )
    return f"æ˜ç™½äº†ï¼Œå·²æ ¹æ®æ‚¨é€‰æ‹©çš„â€œ{ov}â€è¿›ä¸€æ­¥ç­›é€‰ï¼š{pre_total} â†’ {post_total} æ¡ã€‚"


def _prepend_selection_summary(message: str, selection_summary: Optional[str]) -> str:
    if selection_summary:
        return f"{selection_summary}\n\n{message}"
    return message


def _filter_out_noop_options(options: List[Dict[str, Any]], all_ids: set) -> List[Dict[str, Any]]:
    """
    Remove options that do not narrow the candidate set at all (ids == all_ids).
    This prevents "23 â†’ 23" loops where the user keeps seeing (and selecting) no-op buckets.
    """
    if not options or not all_ids:
        return options or []
    out: List[Dict[str, Any]] = []
    for o in options:
        ids = o.get("ids")
        if isinstance(ids, list) and ids and set(ids) == all_ids:
            continue
        out.append(o)
    return out


class ChatMessage(BaseModel):
    """èŠå¤©æ¶ˆæ¯"""
    role: str = Field(..., description="æ¶ˆæ¯è§’è‰²ï¼šuser æˆ– assistant")
    content: str = Field(..., description="æ¶ˆæ¯å†…å®¹")


class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚"""
    message: str
    history: Optional[List[ChatMessage]] = Field(default_factory=list)
    logic: Optional[str] = "AND"  # AND or OR
    max_results: Optional[int] = 5
    session_id: Optional[str] = "default"  # ä¼šè¯IDï¼Œç”¨äºå¤šè½®å¯¹è¯


class ChatResponse(BaseModel):
    """èŠå¤©å“åº”"""
    message: str
    results: Optional[List[dict]] = None  # æœç´¢ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
    options: Optional[List[dict]] = None  # é€‰æ‹©é¢˜é€‰é¡¹ï¼ˆå¦‚æœæœ‰ï¼‰
    needs_choice: Optional[bool] = False  # æ˜¯å¦éœ€è¦ç”¨æˆ·é€‰æ‹©
    session_id: Optional[str] = "default"  # ä¼šè¯ID


@router.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    èŠå¤©æ¥å£
    
    é›†æˆæ„å›¾ç†è§£å’Œå¯¹è¯ç®¡ç†
    æ”¯æŒå¤šè½®å¯¹è¯å’Œé€‰æ‹©é¢˜å¼•å¯¼
    """
    # è·å–æœåŠ¡å®ä¾‹
    search_service = get_search_service()
    llm_service = get_llm_service()
    question_service = get_question_service()
    conversation_manager = get_conversation_manager()
    
    # è·å–æˆ–åˆ›å»ºå¯¹è¯çŠ¶æ€
    session_id = request.session_id or "default"
    conv_state = conversation_manager.get_or_create_state(session_id)
    
    # è·å–ç”¨æˆ·æŸ¥è¯¢
    query = request.message.strip()
    if not query:
        return ChatResponse(
            message="è¯·è¾“å…¥æ‚¨è¦æŸ¥æ‰¾çš„ç”µè·¯å›¾å…³é”®è¯ï¼Œä¾‹å¦‚ï¼šä¸œé£å¤©é¾™ä»ªè¡¨é’ˆè„šå›¾",
            session_id=session_id
        )
    
    # æ£€æµ‹ç”¨æˆ·æ˜¯å¦é‡æ–°è¡¨è¾¾éœ€æ±‚ï¼ˆå¦‚"æˆ‘è¦æ‰¾XXX"ã€"æ‰¾ä¸€ä¸‹XXX"ç­‰ï¼‰
    # æ³¨æ„ï¼šä¸è¦å°†"æˆ‘è¦ä¸€ä¸ªXXX"è¯¯åˆ¤ä¸ºé‡ç½®å…³é”®è¯
    reset_keywords = ["æˆ‘è¦æ‰¾", "æ‰¾ä¸€ä¸‹", "æœç´¢", "æŸ¥æ‰¾", "é‡æ–°", "æ¢ä¸€ä¸ª"]
    # æ£€æŸ¥æ˜¯å¦æ˜¯é‡ç½®å…³é”®è¯ï¼ˆæ’é™¤"æˆ‘è¦ä¸€ä¸ª"è¿™ç§æƒ…å†µï¼‰
    is_new_query = False
    if conv_state.state != ConversationStateEnum.INITIAL:
        for keyword in reset_keywords:
            if keyword in query:
                is_new_query = True
                break
        # ç‰¹æ®Šå¤„ç†ï¼š"æˆ‘è¦ä¸€ä¸ªXXX"ä¸åº”è¯¥è§¦å‘é‡ç½®
        if "æˆ‘è¦ä¸€ä¸ª" in query or "æˆ‘è¦ä¸ª" in query:
            is_new_query = False
    
    # å¦‚æœç”¨æˆ·é‡æ–°è¡¨è¾¾éœ€æ±‚ï¼Œé‡ç½®å¯¹è¯çŠ¶æ€
    if is_new_query:
        conv_state.clear()
        # æå–æ–°çš„æŸ¥è¯¢ï¼ˆç§»é™¤é‡ç½®å…³é”®è¯ï¼‰
        for keyword in reset_keywords:
            query = query.replace(keyword, "").strip()
        if not query:
            return ChatResponse(
                message="è¯·è¾“å…¥æ‚¨è¦æŸ¥æ‰¾çš„ç”µè·¯å›¾å…³é”®è¯ï¼Œä¾‹å¦‚ï¼šä¸œé£å¤©é¾™ä»ªè¡¨é’ˆè„šå›¾",
                session_id=session_id
            )
    
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    conv_state.add_message("user", query)

    # å¦‚æœä¸Šä¸€è½®æ˜¯â€œæ”¾å®½å…³é”®è¯åæ‰¾åˆ°è¿‘ä¼¼ç»“æœâ€çš„ç¡®è®¤æ€ï¼Œè¿™ä¸€è½®ä¼˜å…ˆå¤„ç†â€œéœ€è¦/ä¸éœ€è¦â€
    if conv_state.state == ConversationStateEnum.NEEDS_CONFIRM:
        user_confirm = (query or "").strip()
        yes_set = {"éœ€è¦", "è¦", "å¥½çš„", "å¥½", "æ˜¯", "å¯ä»¥", "è¡Œ", "ok", "OK"}
        no_set = {"ä¸éœ€è¦", "ä¸è¦", "ä¸ç”¨", "å¦", "ä¸", "ç®—äº†"}

        if user_confirm in yes_set:
            # å…³é”®ä¿®å¤ï¼š
            # ç”¨æˆ·å›å¤â€œéœ€è¦â€æ—¶ï¼Œå¿…é¡»æ²¿ç”¨â€œæ”¾å®½åå¾—åˆ°çš„ç»“æœé›†â€ï¼Œç»ä¸èƒ½å†æ¬¡è·‘æ„å›¾è¯†åˆ«/é‡æ–°æœç´¢
            # å¦åˆ™ä¼šæŠŠâ€œä¹˜é¾™H7ç”µè·¯å›¾â€æ‰©å¤§æˆâ€œä¸œé£æŸ³æ±½ä¸€å †ç³»åˆ—â€çš„æ— å…³é›†åˆã€‚

            # é‡è¦ï¼šä¸è¦æŠŠ current_query è¦†ç›–æˆâ€œéœ€è¦â€
            query = (conv_state.current_query or "").strip() or query

            # ç›´æ¥å¤ç”¨ä¸Šä¸€è½®çš„æ”¾å®½ç»“æœï¼ˆæœ¬èº«å°±æ˜¯ used_keywords çš„ AND äº¤é›†ï¼‰
            scored_results = search_service.deduplicate_results(conv_state.search_results or [])
            conv_state.search_results = scored_results

            # å¤ç”¨ä¸Šä¸€è½®çš„æ„å›¾ï¼ˆè‹¥å­˜åœ¨ï¼‰ï¼Œç”¨äºç”Ÿæˆæ›´åˆç†çš„é€‰æ‹©é¢˜ä¸Šä¸‹æ–‡
            intent_result = conv_state.intent_result

            meta = conv_state.relax_meta or {}
            used = meta.get("used_keywords") or []
            try:
                print(f"[DEBUG] confirm=YES, relaxed used_keywords: {used}")
            except Exception:
                pass

            # å…³é”®ä¿®å¤ï¼š
            # è¿›å…¥â€œæ”¾å®½åç»§ç»­â€çš„åç»­é€‰æ‹©é¢˜æ—¶ï¼Œcurrent_query å¿…é¡»æ”¹æˆ used_keywordsï¼Œ
            # å¦åˆ™é—®é¢˜æ–‡æœ¬/åç»­åˆ†ç»„ä¼šç»§ç»­å¼•ç”¨è¢«å‰”é™¤çš„å…³é”®è¯ï¼ˆä¾‹å¦‚ï¼šåº†é¾„=0 ä½† UI ä»è¯´â€œåº†é¾„ç›¸å…³â€ï¼‰ã€‚
            relaxed_query = " ".join([str(x).strip() for x in used if str(x).strip()])
            if relaxed_query:
                query = relaxed_query
                conv_state.current_query = relaxed_query

            # æ ‡è®°ï¼šåç»­ä¸è¦å†åšæ„å›¾è§£æ/æœç´¢ï¼Œåªåšé€‰æ‹©é¢˜/å±•ç¤º
            skip_search = True
            force_choose = True
            conv_state.update_state(ConversationStateEnum.SEARCHING)
        elif user_confirm in no_set:
            conv_state.update_state(ConversationStateEnum.COMPLETED)
            msg = "å¥½çš„ã€‚å¦‚éœ€ç»§ç»­ï¼Œè¯·ç›´æ¥è¾“å…¥æ–°çš„å…³é”®è¯é‡æ–°æœç´¢ã€‚"
            conv_state.add_message("assistant", msg)
            return ChatResponse(message=msg, session_id=session_id)
        else:
            msg = "æˆ‘å¯ä»¥åŸºäºå·²æ‰¾åˆ°çš„ç›¸å…³èµ„æ–™ç»§ç»­ä¸ºæ‚¨ç­›é€‰ã€‚è¯·å›å¤â€œéœ€è¦â€ç»§ç»­ï¼Œæˆ–å›å¤â€œä¸éœ€è¦â€é‡æ–°æœç´¢ã€‚"
            conv_state.add_message("assistant", msg)
            return ChatResponse(message=msg, session_id=session_id)
    
    # æ£€æµ‹ç”¨æˆ·æ˜¯å¦åœ¨å›ç­”â€œé€‰é¡¹æ ‡ç­¾â€ï¼ˆæ”¯æŒ A..Z, AA.. ç­‰åŠ¨æ€æ‰©å±•æ ‡ç­¾ï¼‰
    user_input_upper = (query or "").upper().strip()
    is_option_selection = False
    if conv_state.state == ConversationStateEnum.NEEDS_CHOICE and conv_state.current_options:
        labels = {str(o.get("label", "")).upper().strip() for o in conv_state.current_options if o.get("label")}
        if user_input_upper in labels:
            is_option_selection = True
    
    # å¦‚æœå½“å‰çŠ¶æ€æ˜¯ç­‰å¾…é€‰æ‹©ï¼Œä¸”ç”¨æˆ·è¾“å…¥æ˜¯é€‰é¡¹ï¼Œå¤„ç†é€‰æ‹©
    if conv_state.state == ConversationStateEnum.NEEDS_CHOICE and is_option_selection:
        # è§£æç”¨æˆ·é€‰æ‹©
        if conv_state.current_options:
            # æ‰¾åˆ°å¯¹åº”çš„é€‰é¡¹
            selected_option = None
            for option in conv_state.current_options:
                if option.get('label') == user_input_upper:
                    selected_option = option
                    break
            
            if selected_option:
                # åŸºäºé€‰æ‹©ç­›é€‰ç»“æœ
                option_type = selected_option.get('type')
                option_value = selected_option.get('name')
                
                pre_filter_total = len(conv_state.search_results or [])
                filtered_results = conv_state.search_results
                # å…³é”®ä¿®å¤ï¼šå¦‚æœé€‰é¡¹è‡ªå¸¦ idsï¼Œä¼˜å…ˆæŒ‰ ids ç²¾ç¡®è¿‡æ»¤ï¼ˆé¿å…â€œé€‰NTå´æ··å…¥MT/Nâ€ç­‰é—®é¢˜ï¼‰
                opt_ids = selected_option.get("ids")
                if isinstance(opt_ids, list) and opt_ids:
                    try:
                        id_set = {int(x) for x in opt_ids}
                    except Exception:
                        id_set = {x for x in opt_ids}
                    filtered_results = [r for r in (filtered_results or []) if r.diagram.id in id_set]
                if option_type == "brand":
                    conv_state.add_filter("brand", option_value or "")
                    filtered_results = search_service.filter_by_hierarchy(
                        filtered_results, brand=option_value
                    )
                elif option_type == "model":
                    conv_state.add_filter("model", option_value or "")
                    filtered_results = search_service.filter_by_hierarchy(
                        filtered_results, model=option_value
                    )
                elif option_type == "type":
                    conv_state.add_filter("type", option_value or "")
                    filtered_results = search_service.filter_by_hierarchy(
                        filtered_results, diagram_type=option_value
                    )
                elif option_type == "variant":
                    conv_state.add_filter("variant", option_value or "")
                    # è½¦å‹å˜ä½“ï¼šæŒ‰æ–‡ä»¶åå‰ç¼€ç²¾ç¡®åˆ†ç»„ï¼ˆä¾‹å¦‚ â€œä¸œé£å¤©é¾™KL_6x4ç¯å«è½¦â€ï¼‰
                    base = (option_value or "").strip()
                    for suf in (" ç³»åˆ—", "ç³»åˆ—"):
                        if base.endswith(suf):
                            base = base[: -len(suf)].strip()
                            break
                    next_filtered = []
                    for r in filtered_results:
                        k = variant_key_for_query(r.diagram.file_name or "", conv_state.current_query or "")
                        if k and k == base:
                            next_filtered.append(r)
                    filtered_results = next_filtered
                elif option_type == "brand_model":
                    # å“ç‰Œ+å‹å·ç»„åˆï¼šè§£æé€‰é¡¹å€¼ï¼ˆå¦‚"ä¸œé£ å¤©é¾™KL"ã€"ä¸œé£ DOC"ã€"ä¸œé£ VEC"ç­‰ï¼‰
                    brand, model = search_service._parse_brand_model(option_value)
                    if brand and model:
                        conv_state.add_filter("brand", brand)
                        conv_state.add_filter("model", model)
                        # å…ˆæŒ‰å“ç‰Œç­›é€‰
                        filtered_results = search_service.filter_by_hierarchy(
                            filtered_results, brand=brand
                        )
                        # å†æŒ‰å‹å·ç­›é€‰ï¼ˆæ”¯æŒå±‚çº§è·¯å¾„åŒ¹é…ï¼‰
                        if filtered_results:
                            filtered_diagrams = HierarchyUtil.filter_by_model(
                                [r.diagram for r in filtered_results], model
                            )
                            filtered_ids = {d.id for d in filtered_diagrams}
                            filtered_results = [r for r in filtered_results if r.diagram.id in filtered_ids]
                    elif brand:
                        conv_state.add_filter("brand", brand)
                        filtered_results = search_service.filter_by_hierarchy(
                            filtered_results, brand=brand
                        )
                elif option_type == "result":
                    conv_state.add_filter("result", option_value or user_input_upper)
                    # ç›´æ¥é€‰æ‹©æŸä¸€ä»½èµ„æ–™ï¼ˆæ–‡ä»¶ï¼‰
                    target_id = selected_option.get("id")
                    if target_id is not None:
                        filtered_results = [r for r in (filtered_results or []) if r.diagram.id == target_id]
                
                # æ›´æ–°å¯¹è¯çŠ¶æ€
                conv_state.search_results = filtered_results
                conv_state.current_options = []
                conv_state.option_type = None
                # æ”¯æŒé…ç½®/è½´å‹ç­›é€‰ï¼ˆå¦‚ 6x4 ç‰µå¼•è½¦ï¼‰
                if option_type == "config":
                    conv_state.add_filter("config", option_value or "")
                    # åŸºäºè§„èŒƒåŒ–æ–‡æœ¬åŒ…å«åŒ¹é…
                    from backend.app.services.search_service import SearchService
                    target = SearchService._norm_text(option_value)
                    if target:
                        next_filtered = []
                        for r in filtered_results:
                            d = r.diagram
                            blob = SearchService._diagram_blob(d)
                            if target in blob:
                                next_filtered.append(r)
                        filtered_results = next_filtered
                        conv_state.search_results = filtered_results
                
                # æ£€æŸ¥ç­›é€‰åçš„ç»“æœæ•°é‡
                if not filtered_results:
                    conv_state.update_state(ConversationStateEnum.COMPLETED)
                    conv_state.add_message("assistant", f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸ã€Œ{option_value}ã€ç›¸å…³çš„ç”µè·¯å›¾ã€‚è¯·å°è¯•å…¶ä»–é€‰é¡¹æˆ–é‡æ–°æœç´¢ã€‚")
                    return ChatResponse(
                        message=f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸ã€Œ{option_value}ã€ç›¸å…³çš„ç”µè·¯å›¾ã€‚è¯·å°è¯•å…¶ä»–é€‰é¡¹æˆ–é‡æ–°æœç´¢ã€‚",
                        session_id=session_id
                    )
                
                # ç»Ÿä¸€çš„ç­›é€‰æ‘˜è¦ï¼šæ— è®ºç»§ç»­æé—®è¿˜æ˜¯ç›´æ¥å‡ºç»“æœï¼Œéƒ½å…ˆå†™æ¸…â€œæ¡æ•°å˜åŒ– + å½“å‰ç­›é€‰æ¡ä»¶â€
                selection_summary = _build_selection_summary(
                    option_value=option_value or user_input_upper,
                    pre_total=pre_filter_total,
                    post_total=len(filtered_results),
                    filter_history=conv_state.filter_history,
                )
                # å…³é”®ä¿æŠ¤ï¼šç­›é€‰åç»“æœä¸èƒ½æ¯”ç­›é€‰å‰æ›´å¤šï¼ˆå¦åˆ™è¯´æ˜é€‰é¡¹count/idsæˆ–ç­›é€‰é€»è¾‘ä¸ä¸€è‡´ï¼‰
                if len(filtered_results) > pre_filter_total:
                    print(f"[WARN] selection increased results: {pre_filter_total} -> {len(filtered_results)}; option={option_type}:{option_value}")
                
                # å¦‚æœç­›é€‰åç»“æœâ‰¤5ä¸ªï¼Œç›´æ¥è¿”å›ç»“æœ
                max_results = request.max_results or 5
                if len(filtered_results) <= max_results:
                    formatted_results = []
                    for result in filtered_results:
                        formatted_results.append({
                            "id": result.diagram.id,
                            "file_name": result.diagram.file_name,
                            "hierarchy_path": " -> ".join(result.diagram.hierarchy_path),
                            "score": round(result.score, 2),
                            "brand": result.diagram.brand,
                            "model": result.diagram.model,
                            "diagram_type": result.diagram.diagram_type
                        })
                    
                    # è‹¥ç”¨æˆ·æŸ¥è¯¢åŒ…å«â€œç”µè·¯å›¾â€ï¼Œä¸”ç­›é€‰ååªå‰©å•ä¸€å›¾çº¸ç±»å‹ï¼Œåˆ™åŠ ä¸€æ®µç¡®è®¤è¯æœ¯ï¼ˆæ›´è´´è¿‘ä¸šåŠ¡æœŸæœ›ï¼‰
                    preface = ""
                    try:
                        q0 = (conv_state.current_query or "")
                        unique_types = {r.diagram.diagram_type for r in filtered_results if getattr(r.diagram, "diagram_type", None)}
                        if ("ç”µè·¯å›¾" in q0) and len(unique_types) == 1:
                            only_type = next(iter(unique_types))
                            preface = f"è¡¥å……è¯´æ˜ï¼šæŸ¥çœ‹åŒ…å«ç”µè·¯å›¾çš„æ•°æ®ï¼Œå‘ç°{pre_filter_total}æ¡æ•°æ®ä¸­å›¾çº¸ç±»å‹åªæœ‰â€œ{only_type}â€ï¼Œæˆ‘ç›´æ¥æŠŠç»“æœåˆ—å‡ºæ¥ï¼š\n\n"
                    except Exception:
                        preface = ""

                    message = _prepend_selection_summary(preface + f"å·²ä¸ºæ‚¨æ‰¾åˆ°ä»¥ä¸‹ç”µè·¯å›¾ï¼š\n\n", selection_summary)
                    for i, result in enumerate(formatted_results, 1):
                        message += f"{i}. [ID: {result['id']}] {result['file_name']}\n"
                        message += f"   è·¯å¾„: {result['hierarchy_path']}\n"
                        if result['brand'] or result['model']:
                            attrs = []
                            if result['brand']:
                                attrs.append(f"å“ç‰Œ: {result['brand']}")
                            if result['model']:
                                attrs.append(f"å‹å·: {result['model']}")
                            if result['diagram_type']:
                                attrs.append(f"ç±»å‹: {result['diagram_type']}")
                            if attrs:
                                message += f"   {', '.join(attrs)}\n"
                        message += "\n"
                    
                    conv_state.update_state(ConversationStateEnum.COMPLETED)
                    conv_state.add_message("assistant", message)
                    
                    return ChatResponse(
                        message=message,
                        results=formatted_results,
                        needs_choice=False,
                        session_id=session_id
                    )
                
                # å¦‚æœç­›é€‰åç»“æœä»ç„¶>5ä¸ªï¼Œç»§ç»­ç”Ÿæˆé€‰æ‹©é¢˜
                # å…³é”®ä¿®å¤ï¼š
                # å·²ç»æœ‰ filtered_resultsï¼ˆå€™é€‰é›†ï¼‰äº†ï¼Œè¿™é‡Œç»ä¸èƒ½æŠŠ query æ”¹æˆé€‰é¡¹æ–‡æœ¬å†å»è·‘ä¸€æ¬¡å…¨é‡ AND æœç´¢ï¼Œ
                # å¦åˆ™ä¼šå‡ºç°â€œä¸Šä¸€è½®æ˜¾ç¤º 7 æ¡/33 æ¡ï¼Œä¸‹ä¸€è½®å´åªå‰© 1 æ¡ç”šè‡³ 0 æ¡â€çš„æ·±åº¦æœç´¢é”™ä¹±ã€‚
                scored_results = filtered_results
                skip_search = True
                force_choose = True
                # åŒæ—¶é¿å…æŠŠ current_query è¦†ç›–æˆç”¨æˆ·è¾“å…¥çš„â€œé€‰é¡¹æ ‡ç­¾(A/B/AA...)â€
                query = (conv_state.current_query or "").strip() or (option_value or "").strip() or query
            else:
                conv_state.add_message("assistant", "æŠ±æ­‰ï¼Œæ— æ³•è¯†åˆ«æ‚¨çš„é€‰æ‹©ã€‚è¯·é‡æ–°é€‰æ‹©æˆ–è¾“å…¥é€‰é¡¹åç§°ã€‚")
                return ChatResponse(
                    message="æŠ±æ­‰ï¼Œæ— æ³•è¯†åˆ«æ‚¨çš„é€‰æ‹©ã€‚è¯·é‡æ–°é€‰æ‹©æˆ–è¾“å…¥é€‰é¡¹åç§°ã€‚",
                    session_id=session_id
                )
        else:
            # æ²¡æœ‰é€‰é¡¹æ•°æ®ï¼Œé‡æ–°æœç´¢
            pass
    
    # å¦‚æœç”¨æˆ·è¾“å…¥æ˜¯æ–‡æœ¬é€‰é¡¹åç§°ï¼Œä¹Ÿå°è¯•åŒ¹é…
    elif conv_state.state == ConversationStateEnum.NEEDS_CHOICE and conv_state.current_options:
        # å°è¯•åŒ¹é…é€‰é¡¹åç§°
        matched_option = None
        for option in conv_state.current_options:
            if query.lower() in option.get('name', '').lower() or \
               option.get('name', '').lower() in query.lower():
                matched_option = option
                break
        
        if matched_option:
            # å¤„ç†åŒ¹é…çš„é€‰é¡¹
            option_type = matched_option.get('type')
            option_value = matched_option.get('name')
            
            filtered_results = conv_state.search_results
            # å…³é”®ä¿®å¤ï¼šå¦‚æœé€‰é¡¹è‡ªå¸¦ idsï¼Œä¼˜å…ˆæŒ‰ ids ç²¾ç¡®è¿‡æ»¤
            opt_ids = matched_option.get("ids")
            if isinstance(opt_ids, list) and opt_ids:
                try:
                    id_set = {int(x) for x in opt_ids}
                except Exception:
                    id_set = {x for x in opt_ids}
                filtered_results = [r for r in (filtered_results or []) if r.diagram.id in id_set]
            pre_filter_total = len(conv_state.search_results or [])
            if option_type == "brand":
                conv_state.add_filter("brand", option_value or "")
                filtered_results = search_service.filter_by_hierarchy(
                    filtered_results, brand=option_value
                )
            elif option_type == "model":
                conv_state.add_filter("model", option_value or "")
                filtered_results = search_service.filter_by_hierarchy(
                    filtered_results, model=option_value
                )
            elif option_type == "type":
                conv_state.add_filter("type", option_value or "")
                filtered_results = search_service.filter_by_hierarchy(
                    filtered_results, diagram_type=option_value
                )
            elif option_type == "variant":
                conv_state.add_filter("variant", option_value or "")
                # è½¦å‹å˜ä½“ï¼šæŒ‰æ–‡ä»¶åå‰ç¼€ç²¾ç¡®åˆ†ç»„ï¼ˆä¾‹å¦‚ â€œä¸œé£å¤©é¾™KL_6x4ç¯å«è½¦â€ï¼‰
                base = (option_value or "").strip()
                for suf in (" ç³»åˆ—", "ç³»åˆ—"):
                    if base.endswith(suf):
                        base = base[: -len(suf)].strip()
                        break
                next_filtered = []
                for r in filtered_results:
                    k = variant_key_for_query(r.diagram.file_name or "", conv_state.current_query or "")
                    if k and k == base:
                        next_filtered.append(r)
                filtered_results = next_filtered
            elif option_type == "brand_model":
                # å“ç‰Œ+å‹å·ç»„åˆï¼šè§£æé€‰é¡¹å€¼ï¼ˆå¦‚"ä¸œé£ DOC"ã€"ä¸œé£ VEC"ç­‰ï¼‰
                brand, model = search_service._parse_brand_model(option_value)
                if brand and model:
                    conv_state.add_filter("brand", brand)
                    conv_state.add_filter("model", model)
                    # å…ˆæŒ‰å“ç‰Œç­›é€‰
                    filtered_results = search_service.filter_by_hierarchy(
                        filtered_results, brand=brand
                    )
                    # å†æŒ‰å‹å·ç­›é€‰ï¼ˆæ”¯æŒå±‚çº§è·¯å¾„åŒ¹é…ï¼‰
                    if filtered_results:
                        filtered_diagrams = HierarchyUtil.filter_by_model(
                            [r.diagram for r in filtered_results], model
                        )
                        filtered_ids = {d.id for d in filtered_diagrams}
                        filtered_results = [r for r in filtered_results if r.diagram.id in filtered_ids]
                elif brand:
                    conv_state.add_filter("brand", brand)
                    filtered_results = search_service.filter_by_hierarchy(
                        filtered_results, brand=brand
                    )
            elif option_type == "result":
                conv_state.add_filter("result", option_value or query)
                target_id = matched_option.get("id")
                if target_id is not None:
                    filtered_results = [r for r in (filtered_results or []) if r.diagram.id == target_id]
            
            conv_state.search_results = filtered_results
            conv_state.current_options = []
            conv_state.option_type = None
            # æ”¯æŒé…ç½®/è½´å‹ç­›é€‰ï¼ˆå¦‚ 6x4 ç‰µå¼•è½¦ï¼‰
            if option_type == "config":
                conv_state.add_filter("config", option_value or "")
                from backend.app.services.search_service import SearchService
                target = SearchService._norm_text(option_value)
                if target:
                    next_filtered = []
                    for r in filtered_results:
                        d = r.diagram
                        blob = SearchService._diagram_blob(d)
                        if target in blob:
                            next_filtered.append(r)
                    filtered_results = next_filtered
                    conv_state.search_results = filtered_results
            
            # æ£€æŸ¥ç­›é€‰åçš„ç»“æœæ•°é‡
            if not filtered_results:
                conv_state.update_state(ConversationStateEnum.COMPLETED)
                conv_state.add_message("assistant", f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸ã€Œ{option_value}ã€ç›¸å…³çš„ç”µè·¯å›¾ã€‚è¯·å°è¯•å…¶ä»–é€‰é¡¹æˆ–é‡æ–°æœç´¢ã€‚")
                return ChatResponse(
                    message=f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸ã€Œ{option_value}ã€ç›¸å…³çš„ç”µè·¯å›¾ã€‚è¯·å°è¯•å…¶ä»–é€‰é¡¹æˆ–é‡æ–°æœç´¢ã€‚",
                    session_id=session_id
                )
            
            # ç»Ÿä¸€çš„ç­›é€‰æ‘˜è¦ï¼šæ— è®ºç»§ç»­æé—®è¿˜æ˜¯ç›´æ¥å‡ºç»“æœï¼Œéƒ½å…ˆå†™æ¸…â€œæ¡æ•°å˜åŒ– + å½“å‰ç­›é€‰æ¡ä»¶â€
            selection_summary = _build_selection_summary(
                option_value=option_value or query,
                pre_total=pre_filter_total,
                post_total=len(filtered_results),
                filter_history=conv_state.filter_history,
            )
            
            # å¦‚æœç­›é€‰åç»“æœâ‰¤5ä¸ªï¼Œç›´æ¥è¿”å›ç»“æœ
            max_results = request.max_results or 5
            if len(filtered_results) <= max_results:
                formatted_results = []
                for result in filtered_results:
                    formatted_results.append({
                        "id": result.diagram.id,
                        "file_name": result.diagram.file_name,
                        "hierarchy_path": " -> ".join(result.diagram.hierarchy_path),
                        "score": round(result.score, 2),
                        "brand": result.diagram.brand,
                        "model": result.diagram.model,
                        "diagram_type": result.diagram.diagram_type
                    })
                
                # è‹¥ç”¨æˆ·æŸ¥è¯¢åŒ…å«â€œç”µè·¯å›¾â€ï¼Œä¸”ç­›é€‰ååªå‰©å•ä¸€å›¾çº¸ç±»å‹ï¼Œåˆ™åŠ ä¸€æ®µç¡®è®¤è¯æœ¯ï¼ˆæ›´è´´è¿‘ä¸šåŠ¡æœŸæœ›ï¼‰
                preface = ""
                try:
                    q0 = (conv_state.current_query or "")
                    unique_types = {r.diagram.diagram_type for r in filtered_results if getattr(r.diagram, "diagram_type", None)}
                    if ("ç”µè·¯å›¾" in q0) and len(unique_types) == 1:
                        only_type = next(iter(unique_types))
                        # æ³¨æ„ï¼šè¿™é‡Œçš„ pre_filter_total åœ¨â€œæ–‡æœ¬å‘½ä¸­é€‰é¡¹â€åˆ†æ”¯ä¹Ÿåº”å–ç­›é€‰å‰çš„æ€»æ•°
                        preface = f"è¡¥å……è¯´æ˜ï¼šæŸ¥çœ‹åŒ…å«ç”µè·¯å›¾çš„æ•°æ®ï¼Œå‘ç°{pre_filter_total}æ¡æ•°æ®ä¸­å›¾çº¸ç±»å‹åªæœ‰â€œ{only_type}â€ï¼Œæˆ‘ç›´æ¥æŠŠç»“æœåˆ—å‡ºæ¥ï¼š\n\n"
                except Exception:
                    preface = ""

                message = _prepend_selection_summary(preface + f"å·²ä¸ºæ‚¨æ‰¾åˆ°ä»¥ä¸‹ç”µè·¯å›¾ï¼š\n\n", selection_summary)
                for i, result in enumerate(formatted_results, 1):
                    message += f"{i}. [ID: {result['id']}] {result['file_name']}\n"
                    message += f"   è·¯å¾„: {result['hierarchy_path']}\n"
                    if result['brand'] or result['model']:
                        attrs = []
                        if result['brand']:
                            attrs.append(f"å“ç‰Œ: {result['brand']}")
                        if result['model']:
                            attrs.append(f"å‹å·: {result['model']}")
                        if result['diagram_type']:
                            attrs.append(f"ç±»å‹: {result['diagram_type']}")
                        if attrs:
                            message += f"   {', '.join(attrs)}\n"
                    message += "\n"
                
                conv_state.update_state(ConversationStateEnum.COMPLETED)
                conv_state.add_message("assistant", message)
                
                return ChatResponse(
                    message=message,
                    results=formatted_results,
                    needs_choice=False,
                    session_id=session_id
                )
            
            # å¦‚æœç­›é€‰åç»“æœä»ç„¶>5ä¸ªï¼Œç»§ç»­ç”Ÿæˆé€‰æ‹©é¢˜
            scored_results = filtered_results
            skip_search = True
            force_choose = True
            query = (conv_state.current_query or "").strip() or (option_value or "").strip() or query

            if len(filtered_results) > pre_filter_total:
                print(f"[WARN] selection increased results(text-match): {pre_filter_total} -> {len(filtered_results)}; option={option_type}:{option_value}")
    
    # æ‰§è¡Œæ„å›¾ç†è§£ï¼ˆç¡®è®¤æ€â€œéœ€è¦â€ä¼šè·³è¿‡ï¼‰
    if "skip_search" not in locals():
        skip_search = False
    if "intent_result" not in locals():
        intent_result = None
    if not skip_search:
        try:
            intent_result = llm_service.parse_intent(query)
            conv_state.intent_result = intent_result
        except Exception as e:
            print(f"âš ï¸ æ„å›¾ç†è§£å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨å…³é”®è¯æœç´¢")
            # æ„å›¾ç†è§£å¤±è´¥æ—¶ï¼Œç»§ç»­ä½¿ç”¨å…³é”®è¯æœç´¢

    # å…³é”®ä¿®å¤ï¼š
    # LLM å¯èƒ½ä¼šâ€œæ¨æ–­â€å“ç‰Œ/ç±»å‹ï¼ˆä¾‹å¦‚ JH6 -> è§£æ”¾ï¼‰ï¼Œä½†è‹¥ç”¨æˆ·åŸæ–‡æœªå‡ºç°è¯¥å“ç‰Œ/ç±»å‹ï¼Œ
    # å°±ä¸åº”æŠŠå®ƒå½“ä½œç¡¬æ¡ä»¶å‚ä¸æ£€ç´¢/è¿‡æ»¤ï¼Œä¹Ÿä¸åº”åœ¨å›å¤é‡Œå¼ºè¡Œå±•ç¤ºè¯¥è¯ã€‚
    def _is_explicit_brand(brand: str, raw_q: str) -> bool:
        if not brand or not raw_q:
            return False
        if brand in raw_q:
            return True
        # è‹¥ç”¨æˆ·åªå†™äº†ç®€ç§°ï¼Œä¹Ÿè§†ä¸ºæ˜¾å¼ï¼ˆä¾‹å¦‚ â€œé‡æ±½â€ å‘½ä¸­ â€œä¸­å›½é‡æ±½/é‡æ±½è±ªç€šâ€ï¼‰
        raw = raw_q
        hints = ["ä¸œé£", "è§£æ”¾", "é‡æ±½", "ä¸­å›½é‡æ±½", "ä¸€æ±½", "ä¸Šæ±½", "å¤§é€š", "æ¬§æ›¼", "ä¹˜é¾™", "çº¢å²©", "æ°ç‹®", "è±ªç€š", "è±ªæ±‰", "è±ªæ²ƒ", "ç¦ç”°"]
        for h in hints:
            if h in brand and h in raw:
                return True
        return False

    def _is_explicit_type(diagram_type: str, raw_q: str) -> bool:
        if not diagram_type or not raw_q:
            return False
        # åªè¦ç”¨æˆ·åŸæ–‡æåˆ°äº†è¯¥ç±»å‹æˆ–å…¶å¸¸è§ä¸Šä½è¯ï¼Œå°±ç®—æ˜¾å¼
        type_hints = ["ECU", "ä»ªè¡¨", "æ•´è½¦", "é’ˆè„š", "é’ˆè§’", "æ¥çº¿", "çº¿è·¯", "ç”µè·¯å›¾", "çº¿è·¯å›¾", "æ¥çº¿å›¾", "é’ˆè„šå›¾", "é’ˆè§’å›¾", "æ•´è½¦ç”µè·¯å›¾", "ä»ªè¡¨å›¾", "ECUå›¾"]
        return any(t in raw_q for t in type_hints)

    if intent_result:
        raw_q = (query or "")
        if intent_result.brand and not _is_explicit_brand(intent_result.brand, raw_q):
            intent_result.brand = None
        if intent_result.diagram_type and not _is_explicit_type(intent_result.diagram_type, raw_q):
            intent_result.diagram_type = None
    
    # æ›´æ–°å¯¹è¯çŠ¶æ€
    conv_state.update_state(ConversationStateEnum.SEARCHING)
    conv_state.current_query = query
    
    # è®°å½•ç”¨æˆ·å·²æŒ‡å®šçš„å“ç‰Œ/ç±»å‹ï¼Œç”¨äºåç»­è¿‡æ»¤å’Œé¿å…é‡å¤æé—®
    brand_already_specified = (intent_result.has_brand() and _is_explicit_brand(intent_result.brand, conv_state.current_query or "")) if intent_result else False
    type_already_specified = (intent_result.has_diagram_type() and _is_explicit_type(intent_result.diagram_type, conv_state.current_query or "")) if intent_result else False
    brand_tokens = []
    if brand_already_specified and intent_result.brand:
        brand_tokens.append(intent_result.brand)
        base_brand_hints = ["ä¸œé£", "è§£æ”¾", "é‡æ±½", "æ¬§æ›¼", "ä¹˜é¾™", "æ°ç‹®", "è±ªç€š", "è±ªæ±‰", "å¤§é€š"]
        for hint in base_brand_hints:
            if hint in intent_result.brand:
                brand_tokens.append(hint)

    # æ‰§è¡Œæœç´¢ï¼ˆç¡®è®¤æ€â€œéœ€è¦â€ä¼šè·³è¿‡è¿™é‡Œï¼Œå› ä¸º scored_results å·²ç»å­˜åœ¨ä¸” skip_search=Trueï¼‰
    logic = request.logic or "AND"
    max_results = request.max_results or 5
    # é€‰é¡¹æ•°é‡ä¸è¦ç¡¬ç»‘å®š max_resultsï¼šç”¨æˆ·è¦æ±‚â€œä¸œé£å¤©é¾™â€è¿™ç±»å¤§ç»“æœé›†è¦å±•ç¤ºæ›´å¤šåˆ†ç±»ä¾›é€‰æ‹©
    max_options = max(5, min(15, max_results * 3))
    if "force_choose" not in locals():
        force_choose = False
    
    if (not skip_search) and ("scored_results" not in locals()):
        # ä½¿ç”¨æ„å›¾ç†è§£ç»“æœè¿›è¡Œæœç´¢
        if intent_result:
            scored_results = search_service.search_with_intent(
                intent_result=intent_result,
                logic=logic,
                max_results=1000,  # è·å–è¶³å¤Ÿå¤šçš„ç»“æœç”¨äºåˆ†æ
                use_fuzzy=True
            )
        else:
            # é™çº§ä¸ºå…³é”®è¯æœç´¢
            scored_results = search_service.search(
                query=query,
                logic=logic,
                max_results=1000,
                use_fuzzy=True
            )

    # ç¡®è®¤æ€â€œéœ€è¦â€ä¼šè·³è¿‡æœç´¢ï¼šè¿™é‡Œç¡®ä¿ scored_results ä¸€å®šå­˜åœ¨ï¼Œé¿å…åç»­é€»è¾‘è·‘å
    if skip_search and ("scored_results" not in locals()):
        scored_results = conv_state.search_results or []
    
    # ä¸¥æ ¼ ANDï¼ˆç”¨æˆ·è¦æ±‚Aï¼‰ï¼šå¿…é¡»åœ¨â€œæ–‡ä»¶åâ€ä¸­åŒæ—¶å‘½ä¸­æ‰€æœ‰å…³é”®è¯ç»„
    strict_filename_failed = False
    strict_removed_terms: List[str] = []
    if logic.upper() == "AND" and scored_results and not skip_search:
        strict_stats = search_service.strict_filename_and_stats(query=query, intent_result=intent_result)
        if (strict_stats.get("and_count") or 0) <= 0:
            strict_filename_failed = True
            term_counts = strict_stats.get("term_counts") or {}
            strict_removed_terms = [t for t, c in term_counts.items() if int(c) <= 0]

    # å¦‚æœ AND æ— ç»“æœï¼ˆæˆ–ä¸¥æ ¼æ–‡ä»¶åANDå¤±è´¥ï¼‰ï¼šæŒ‰ä¸šåŠ¡è§„åˆ™åšâ€œé€æ­¥æ”¾å®½å…³é”®è¯â€çš„å…œåº•ï¼›ä»…åœ¨æ ¸å¿ƒå…³é”®è¯å¾ˆå°‘æ—¶å†å…è®¸ AND->OR
    if (not scored_results or strict_filename_failed) and logic.upper() == "AND" and not skip_search:
        extracted_keywords = search_service._extract_keywords(query)
        core_kw_count = len([k for k in extracted_keywords if k and len(k.strip()) > 0])

        # æ ¸å¿ƒè¯ > 1ï¼šä¸ç›´æ¥ ORï¼Œå…ˆå°è¯•â€œå‰”é™¤ 0 å‘½ä¸­/ä¸å¯ç»„åˆå…³é”®è¯â€çš„ AND æ”¾å®½ç­–ç•¥
        if core_kw_count > 1:
            relaxed, meta = search_service.search_and_relax(
                query=query,
                max_results=1000,
                use_fuzzy=True,
                intent_result=intent_result,
                force_remove_terms=strict_removed_terms if strict_filename_failed else None,
            )
            relaxed = search_service.deduplicate_results(relaxed)

            if relaxed:
                used = meta.get("used_keywords") or []
                removed = meta.get("removed_keywords") or []
                # ä¼˜å…ˆå±•ç¤ºâ€œä¸¥æ ¼æ–‡ä»¶åANDæœªå‘½ä¸­â€çš„å…³é”®è¯ï¼ˆæ›´ç¬¦åˆç”¨æˆ·å¿ƒæ™ºï¼‰
                if strict_filename_failed and strict_removed_terms:
                    removed = strict_removed_terms
                # åªæœ‰â€œç¡®å®æ”¾å®½è¿‡â€æ‰è¿›å…¥ç¡®è®¤æ€
                if removed:
                    removed_txt = "ã€".join([str(x) for x in removed if str(x).strip()])
                    # phraseï¼šæŒ‰ç”¨æˆ·è¦æ±‚Aå±•ç¤ºä¸º â€œä¸œé£å¤©é¾™â€â€œé’ˆè„šâ€ è¿™ç§æ ¼å¼ï¼Œå¹¶å°½é‡å»æ‰è¿‡äºæ³›çš„è¯
                    generic = {"ç”µè·¯å›¾", "çº¿è·¯å›¾", "æ¥çº¿å›¾"}
                    phrase_terms = [str(x) for x in used if str(x).strip() and str(x) not in generic]
                    # å¦‚æœåªå‰©ä¸‹æ³›è¯ï¼ˆå¦‚â€œçº¿è·¯å›¾â€ï¼‰ï¼Œä¹Ÿè¦å±•ç¤ºå‡ºæ¥ï¼›å¦åˆ™ä¼šå˜æˆâ€œç›¸å…³â€ï¼Œç”¨æˆ·ä¼šè§‰å¾—å¾ˆæ€ª
                    shown_terms = phrase_terms if phrase_terms else [str(x) for x in used if str(x).strip()]
                    phrase = "".join([f"â€œ{t}â€" for t in shown_terms]) if shown_terms else "â€œç›¸å…³â€"
                    msg = (
                        "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°**åŒæ—¶åŒ¹é…**æ‚¨å…³é”®è¯çš„ç»“æœï¼ˆANDï¼‰ã€‚\n\n"
                        "å»ºè®®ï¼š\n"
                        "- æ£€æŸ¥å…³é”®è¯æ˜¯å¦è¿‡äºå…·ä½“ï¼ˆå¦‚é’ˆè„šå›¾/ç‰ˆæœ¬å·ï¼‰\n"
                        "- å°è¯•è¡¥å……æˆ–æ›¿æ¢å…³é”®è¯ï¼ˆä¾‹å¦‚ï¼šä»ªè¡¨å›¾/ä»ªè¡¨ç”µè·¯å›¾ï¼‰\n"
                        "- æˆ–è€…å‡å°‘ä¸€ä¸ªé™å®šè¯å†è¯•\n\n"
                        f"åŒæ—¶å·²ä¸ºæ‚¨æ‰©å¤§èŒƒå›´ï¼ˆå»æ‰ä¸åŒ¹é…å…³é”®å­—{removed_txt}ï¼‰ï¼Œ"
                        f"å·²ä¸ºæ‚¨æ‰¾åˆ°â€œ{phrase}â€ç›¸å…³æ•°æ®ï¼Œæ˜¯å¦éœ€è¦ï¼Ÿ\n"
                        "å›å¤éœ€è¦å°±å¯ä»¥è¿›è¡Œé€‰æ‹©é€»è¾‘"
                    )
                    conv_state.search_results = relaxed
                    conv_state.relax_meta = meta
                    conv_state.update_state(ConversationStateEnum.NEEDS_CONFIRM)
                    conv_state.add_message("assistant", msg)
                    return ChatResponse(message=msg, needs_choice=False, session_id=session_id)

                scored_results = relaxed
            else:
                conv_state.update_state(ConversationStateEnum.COMPLETED)
                error_message = (
                    "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°**åŒæ—¶åŒ¹é…**æ‚¨å…³é”®è¯çš„ç»“æœï¼ˆANDï¼‰ã€‚\n\n"
                    "å»ºè®®ï¼š\n"
                    "- æ£€æŸ¥å…³é”®è¯æ˜¯å¦è¿‡äºå…·ä½“ï¼ˆå¦‚é’ˆè„šå›¾/ç‰ˆæœ¬å·ï¼‰\n"
                    "- å°è¯•è¡¥å……æˆ–æ›¿æ¢å…³é”®è¯ï¼ˆä¾‹å¦‚ï¼šä»ªè¡¨å›¾/ä»ªè¡¨ç”µè·¯å›¾ï¼‰\n"
                    "- æˆ–è€…å‡å°‘ä¸€ä¸ªé™å®šè¯å†è¯•"
                )
                conv_state.add_message("assistant", error_message)
                return ChatResponse(message=error_message, session_id=session_id)

        # æ ¸å¿ƒè¯å¾ˆå°‘ï¼ˆ<=1ï¼‰ï¼šå…è®¸ AND->OR å…œåº•
        if not scored_results:
            if intent_result:
                scored_results = search_service.search_with_intent(
                    intent_result=intent_result,
                    logic="OR",
                    max_results=1000,
                    use_fuzzy=True
                )
            else:
                scored_results = search_service.search(
                    query=query,
                    logic="OR",
                    max_results=1000,
                    use_fuzzy=True
                )
    
    # å»é‡
    scored_results = search_service.deduplicate_results(scored_results)

    # å¦‚æœç”¨æˆ·å·²ç»æ˜ç¡®å“ç‰Œ/ç±»å‹ï¼Œä¼˜å…ˆè¿‡æ»¤ï¼Œä½†è¿‡æ»¤ä¸ºç©ºæ—¶ä¸è¦ç›´æ¥æŠ¥é”™ï¼ˆé¿å…â€œå…¶å®æœåˆ°äº†ä½†è¢«å­—æ®µè¿‡æ»¤æ¸…ç©ºâ€ï¼‰
    if intent_result and (brand_already_specified or type_already_specified):
        filtered_results = search_service.filter_by_hierarchy(
            scored_results,
            brand=intent_result.brand if brand_already_specified else None,
            diagram_type=intent_result.diagram_type if type_already_specified else None
        )
        if filtered_results:
            scored_results = filtered_results
        else:
            # é€€ä¸€æ­¥ï¼šå“ç‰Œ/ç±»å‹åˆ†åˆ«å°è¯•ï¼Œèƒ½ä¿ç•™å¤šå°‘ä¿ç•™å¤šå°‘
            alt = []
            if brand_already_specified and intent_result.brand:
                alt = search_service.filter_by_hierarchy(scored_results, brand=intent_result.brand)
            if not alt and type_already_specified and intent_result.diagram_type:
                alt = search_service.filter_by_hierarchy(scored_results, diagram_type=intent_result.diagram_type)
            if alt:
                scored_results = alt
    
    # æ›´æ–°å¯¹è¯çŠ¶æ€ä¸­çš„æœç´¢ç»“æœ
    conv_state.search_results = scored_results
    
    if not scored_results:
        conv_state.update_state(ConversationStateEnum.COMPLETED)
        error_message = (
            "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°**åŒæ—¶åŒ¹é…**æ‚¨å…³é”®è¯çš„ç»“æœï¼ˆANDï¼‰ã€‚\n\n"
            "å»ºè®®ï¼š\n"
            "- æ£€æŸ¥å…³é”®è¯æ˜¯å¦è¿‡äºå…·ä½“ï¼ˆå¦‚é’ˆè„šå›¾/ç‰ˆæœ¬å·ï¼‰\n"
            "- å°è¯•è¡¥å……æˆ–æ›¿æ¢å…³é”®è¯ï¼ˆä¾‹å¦‚ï¼šä»ªè¡¨å›¾/ä»ªè¡¨ç”µè·¯å›¾ï¼‰\n"
            "- æˆ–è€…å‡å°‘ä¸€ä¸ªé™å®šè¯å†è¯•"
        )
        conv_state.add_message("assistant", error_message)
        return ChatResponse(message=error_message, session_id=session_id)
    
    total_found = len(scored_results)
    
    print(f"ğŸ” æœç´¢ç»“æœ: {total_found} ä¸ªï¼Œmax_results: {max_results}")
    
    # å¯¹â€œé’ˆè„š/é’ˆè§’â€è¿™ç±»æŸ¥è¯¢ï¼Œå³ä½¿ç»“æœè¾ƒå°‘ï¼Œä¹Ÿå¼ºåˆ¶èµ°é€‰æ‹©é¢˜ï¼ˆé¿å…ç›´æ¥å 5 æ¡ï¼‰
    if (not force_choose) and re.search(r"(é’ˆè„š|é’ˆè§’)", conv_state.current_query or "") and total_found >= 2:
        force_choose = True

    # å¦‚æœç»“æœè¶…è¿‡5ä¸ªï¼Œæˆ–å¼ºåˆ¶é€‰æ‹©ï¼Œå°è¯•ç”Ÿæˆé€‰æ‹©é¢˜å¼•å¯¼ç”¨æˆ·ç¼©å°èŒƒå›´
    # é‡è¦ï¼šå½“ç»“æœ>5ä¸ªæ—¶ï¼Œå¿…é¡»ç”Ÿæˆé€‰æ‹©é¢˜ï¼Œä¸èƒ½ç›´æ¥è¿”å›ç»“æœ
    if force_choose or total_found > max_results:
        print(f"âœ… ç»“æœæ•°({total_found}) > max_results({max_results})ï¼Œè¿›å…¥é€‰æ‹©é¢˜ç”Ÿæˆé€»è¾‘")
        
        # å¦‚æœæ„å›¾ç†è§£è¯†åˆ«åˆ°äº†å“ç‰Œå’Œç±»å‹ï¼Œå°†å®ƒä»¬æ·»åŠ åˆ°ç­›é€‰å†å²ï¼ˆç”¨äºæŒ‡å¯¼é€‰æ‹©é¢˜ç”Ÿæˆï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œä¸å®é™…ç­›é€‰ç»“æœï¼Œåªæ˜¯è®°å½•ç”¨æˆ·æ„å›¾ï¼Œä»¥ä¾¿ç”Ÿæˆåˆé€‚çš„é€‰æ‹©é¢˜
        temp_filter_history = list(conv_state.filter_history)  # å¤åˆ¶ä¸€ä»½ï¼Œé¿å…ä¿®æ”¹åŸå§‹å†å²
        if intent_result:
            # å¦‚æœè¯†åˆ«åˆ°äº†å“ç‰Œï¼Œæ·»åŠ åˆ°ä¸´æ—¶ç­›é€‰å†å²
            if intent_result.has_brand() and not any(f.get('type') == 'brand' for f in temp_filter_history):
                temp_filter_history.append({
                    "type": "brand",
                    "value": intent_result.brand
                })
            # å¦‚æœè¯†åˆ«åˆ°äº†ç±»å‹ï¼Œæ·»åŠ åˆ°ä¸´æ—¶ç­›é€‰å†å²
            if intent_result.has_diagram_type() and not any(f.get('type') == 'type' for f in temp_filter_history):
                temp_filter_history.append({
                    "type": "type",
                    "value": intent_result.diagram_type
                })
        
        # è·å–å·²ç­›é€‰çš„ç±»å‹ï¼ˆé¿å…é‡å¤æé—®ï¼‰
        excluded_types = [f.get('type') for f in temp_filter_history]
        
        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆä½¿ç”¨ä¸´æ—¶ç­›é€‰å†å²ï¼‰
        context = {
            "filter_history": temp_filter_history,
            "current_query": conv_state.current_query,
            "total_results": total_found,
            "intent_result": {
                "brand": intent_result.brand if intent_result else None,
                "model": intent_result.model if intent_result else None,
                "diagram_type": intent_result.diagram_type if intent_result else None
            } if intent_result else None
        }
        
        # è‹¥ç»“æœæ•°ä¸å¤§ï¼šç›´æ¥è®©ç”¨æˆ·â€œæŒ‰æ–‡ä»¶åç²¾ç¡®é€‰æ‹©â€ï¼ˆæ¯”æŒ‰å“ç‰Œ/å‹å·åˆ†ç»„æ›´ç²¾ç¡®ï¼‰
        question_data = None
        choose_file_threshold = max(max_results, 15)
        if force_choose and total_found <= choose_file_threshold:
            # é€‰é¡¹æ ‡ç­¾æŒ‰éœ€æ‰©å±•ï¼Œé¿å… max_results > 5 æ—¶è¶Šç•Œ
            option_labels = question_service._make_option_labels(min(choose_file_threshold, len(scored_results)))
            formatted_options = []
            for i, r in enumerate(scored_results[:choose_file_threshold]):
                formatted_options.append({
                    "label": option_labels[i],
                    "name": r.diagram.file_name,
                    "count": 1,
                    "type": "result",
                    "id": r.diagram.id,
                })
            question_data = {
                "question": "æ˜ç™½äº†ã€‚è¯·é—®æ‚¨éœ€è¦çš„æ˜¯å“ªä¸€ä»½èµ„æ–™ï¼š",
                "options": formatted_options,
                "option_type": "result",
            }
        else:
            # ç”Ÿæˆé€‰æ‹©é¢˜ï¼ˆä½¿ç”¨LLMç”Ÿæˆè‡ªç„¶çš„é—®é¢˜æ–‡æœ¬ï¼‰
            question_data = question_service.generate_question(
                scored_results,
                min_options=2,
                max_options=max_options,
                excluded_types=excluded_types if excluded_types else None,
                context=context,
                use_llm=True
            )
        
        print(f"ğŸ” question_data: {question_data is not None}")
        
        if question_data:
            print(f"âœ… æˆåŠŸç”Ÿæˆé€‰æ‹©é¢˜ï¼Œé€‰é¡¹æ•°: {len(question_data.get('options', []))}")
            # æ›´æ–°å¯¹è¯çŠ¶æ€
            conv_state.update_state(ConversationStateEnum.NEEDS_CHOICE)
            conv_state.current_options = question_data['options']
            conv_state.option_type = question_data['option_type']
            
            # æ ¼å¼åŒ–æ¶ˆæ¯
            message = question_service.format_question_message(question_data)
            message = _prepend_selection_summary(message, locals().get("selection_summary"))
            
            conv_state.add_message("assistant", message)
            
            return ChatResponse(
                message=message,
                results=None,
                options=question_data['options'],
                needs_choice=True,
                session_id=session_id
            )
        else:
            # æ— æ³•ç”Ÿæˆé€‰æ‹©é¢˜ï¼Œå°è¯•ä»å±‚çº§è·¯å¾„ä¸­æå–æ›´ç»†ç²’åº¦çš„é€‰é¡¹
            # å°è¯•æå–å±‚çº§è·¯å¾„ä¸­çš„ä¸åŒå±‚çº§ä½œä¸ºé€‰é¡¹
            print(f"âš ï¸ generate_questionè¿”å›Noneï¼Œå°è¯•fallbacké€»è¾‘ï¼Œç»“æœæ•°: {total_found}")
            
            # å°è¯•æå–ä¸åŒå±‚çº§çš„é€‰é¡¹
            all_levels = HierarchyUtil.get_all_levels([r.diagram for r in scored_results])
            
            # å°è¯•æ‰¾åˆ°æœ‰å¤šä¸ªé€‰é¡¹çš„å±‚çº§
            best_option_type = None
            best_options = []
            
            # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥ï¼šå“ç‰Œ+å‹å·ç»„åˆ -> å“ç‰Œ -> å‹å· -> ç±»å‹ -> ç±»åˆ«
            # ä¼˜å…ˆå°è¯•ä»å±‚çº§è·¯å¾„ä¸­æå–å“ç‰Œ+å‹å·ç»„åˆ
            try:
                brand_model_options = question_service._extract_options_from_hierarchy(
                    scored_results, max_options
                )
                print(f"âš ï¸ ä»å±‚çº§è·¯å¾„æå–å“ç‰Œ+å‹å·ç»„åˆ: {len(brand_model_options) if brand_model_options else 0} ä¸ªé€‰é¡¹")
                if brand_model_options and len(brand_model_options) >= 2:
                    best_option_type = "brand_model"
                    best_options = brand_model_options
            except Exception as e:
                print(f"âš ï¸ æå–å“ç‰Œ+å‹å·ç»„åˆå¤±è´¥: {str(e)}")
            
            # å¦‚æœå“ç‰Œ+å‹å·ç»„åˆå¤±è´¥ï¼Œå°è¯•å…¶ä»–ç±»å‹
            if not best_options:
                print(f"âš ï¸ å°è¯•å…¶ä»–ç±»å‹é€‰é¡¹ï¼Œå·²æ’é™¤ç±»å‹: {excluded_types}")
                for opt_type, level_set in [("brand", all_levels.get("brands", set())),
                                            ("model", all_levels.get("models", set())),
                                            ("type", all_levels.get("types", set())),
                                            ("category", all_levels.get("categories", set()))]:
                    if opt_type not in (excluded_types or []):
                        print(f"âš ï¸ æ£€æŸ¥ç±»å‹ {opt_type}ï¼Œé€‰é¡¹æ•°: {len(level_set)}")
                        if len(level_set) >= 2:
                            # è½¬æ¢ä¸ºé€‰é¡¹æ ¼å¼ï¼ˆæºå¸¦ç²¾ç¡® idsï¼Œåç»­ç­›é€‰æ‰èƒ½ä¸¥æ ¼æ”¶æ•›ï¼‰
                            options = []
                            for name in list(level_set)[:max_options]:
                                ids = [
                                    r.diagram.id
                                    for r in scored_results
                                    if (
                                        (opt_type == "brand" and r.diagram.brand == name)
                                        or (opt_type == "model" and r.diagram.model == name)
                                        or (opt_type == "type" and r.diagram.diagram_type == name)
                                        or (opt_type == "category" and r.diagram.vehicle_category == name)
                                    )
                                ]
                                options.append({"name": name, "count": len(ids), "ids": ids})
                            # drop no-op buckets (ids == all_ids)
                            options = _filter_out_noop_options(options, {r.diagram.id for r in scored_results})
                            options.sort(key=lambda x: x["count"], reverse=True)
                            print(f"âš ï¸ ç±»å‹ {opt_type} ç”Ÿæˆé€‰é¡¹æ•°: {len(options)}")
                            if len(options) >= 2:
                                best_option_type = opt_type
                                best_options = options[:max_options]
                                break
            
            if best_option_type and best_options:
                # è¿‡æ»¤æ‰â€œå…¨é‡ä¸æ”¶æ•›â€çš„é€‰é¡¹ï¼Œé¿å…ç”¨æˆ·é€‰äº†ä¹Ÿä¸ç¼©å°èŒƒå›´
                best_options = _filter_out_noop_options(best_options, {r.diagram.id for r in scored_results})
                if len(best_options) < 2:
                    best_option_type = None
                    best_options = []
                else:
                    # ç”Ÿæˆé—®é¢˜ï¼ˆä½¿ç”¨LLMæˆ–é»˜è®¤æ¨¡æ¿ï¼‰
                    try:
                        # ä½¿ç”¨å·²ç»åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥çš„ llm_service
                        question_text = llm_service.generate_question_text(
                            option_type=best_option_type,
                            options=best_options,
                            total_count=total_found,
                            context=context
                        )
                    except Exception as e:
                        print(f"âš ï¸ LLMç”Ÿæˆé—®é¢˜å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿")
                        question_text = question_service._generate_question_text(
                            best_option_type, total_found, context
                        )
                    
                    sliced_options = best_options[:max_options]
                    option_labels = question_service._make_option_labels(len(sliced_options))
                    formatted_options = []
                    for i, option in enumerate(sliced_options):
                        opt_ids = option.get("ids") if isinstance(option, dict) else None
                        if isinstance(opt_ids, list) and opt_ids:
                            cnt = len(opt_ids)
                        else:
                            cnt = option.get("count", 0)
                        formatted_options.append({
                            "label": option_labels[i],
                            "name": option['name'],
                            "count": cnt,
                            "type": best_option_type,
                            "ids": opt_ids if isinstance(opt_ids, list) else None,
                        })
                    
                    question_data = {
                        "question": question_text,
                        "options": formatted_options,
                        "option_type": best_option_type
                    }
                    
                    # æ›´æ–°å¯¹è¯çŠ¶æ€
                    conv_state.update_state(ConversationStateEnum.NEEDS_CHOICE)
                    conv_state.current_options = formatted_options
                    conv_state.option_type = best_option_type
                    
                    # æ ¼å¼åŒ–æ¶ˆæ¯
                    message = question_service.format_question_message(question_data)
                    message = _prepend_selection_summary(message, locals().get("selection_summary"))
                    
                    conv_state.add_message("assistant", message)
                    
                    return ChatResponse(
                        message=message,
                        results=None,
                        options=formatted_options,
                        needs_choice=True,
                        session_id=session_id
                    )
            
            # å¦‚æœä»ç„¶æ— æ³•ç”Ÿæˆé€‰æ‹©é¢˜ï¼Œå¼ºåˆ¶å°è¯•ä»å±‚çº§è·¯å¾„ä¸­æå–é€‰é¡¹
            # è¿™æ˜¯æœ€åçš„fallbackï¼Œå¿…é¡»ç”Ÿæˆé€‰æ‹©é¢˜
            if not best_options:
                print(f"âš ï¸ å°è¯•æœ€åçš„fallbackï¼šä»å±‚çº§è·¯å¾„ä¸­æå–ä»»æ„æœ‰åŒºåˆ†åº¦çš„é€‰é¡¹")
                try:
                    # å°è¯•ä»å±‚çº§è·¯å¾„ä¸­æå–ä»»æ„æœ‰åŒºåˆ†åº¦çš„é€‰é¡¹
                    hierarchy_options: Dict[str, set] = {}
                    for result in scored_results:
                        diagram = result.diagram
                        if diagram.hierarchy_path and len(diagram.hierarchy_path) > 2:
                            # å°è¯•æå–å“ç‰Œåé¢çš„å±‚çº§
                            brand_pos = -1
                            if diagram.brand:
                                for i, level in enumerate(diagram.hierarchy_path):
                                    if diagram.brand in level or level == diagram.brand:
                                        brand_pos = i
                                        break
                            
                            if brand_pos != -1 and brand_pos + 1 < len(diagram.hierarchy_path):
                                level_value = diagram.hierarchy_path[brand_pos + 1]
                                level_value_clean = level_value.replace('*', '').strip()
                                if level_value_clean and level_value_clean != diagram.brand:
                                    option_name = f"{diagram.brand} {level_value_clean}"
                                    hierarchy_options.setdefault(option_name, set()).add(diagram.id)
                            else:
                                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å“ç‰Œï¼Œå°è¯•æå–å±‚çº§è·¯å¾„ä¸­çš„å…¶ä»–å±‚çº§
                                for i, level in enumerate(diagram.hierarchy_path):
                                    if i > 0 and level and level != "ç”µè·¯å›¾" and len(level) > 1:
                                        # è·³è¿‡ç¬¬ä¸€ä¸ªå±‚çº§ï¼ˆé€šå¸¸æ˜¯"ç”µè·¯å›¾"ï¼‰
                                        hierarchy_options.setdefault(level, set()).add(diagram.id)
                                        break
                    
                    print(f"âš ï¸ ä»å±‚çº§è·¯å¾„æå–åˆ° {len(hierarchy_options)} ä¸ªé€‰é¡¹")
                    
                    if len(hierarchy_options) >= 2:
                        # è½¬æ¢ä¸ºé€‰é¡¹æ ¼å¼
                        options = [
                            {"name": name, "count": len(ids), "ids": sorted(ids)}
                            for name, ids in sorted(hierarchy_options.items(), key=lambda x: len(x[1]), reverse=True)[:max_options]
                        ]
                        options = _filter_out_noop_options(options, {r.diagram.id for r in scored_results})
                        
                        question_text = question_service._generate_question_text(
                            "brand_model", total_found, context
                        )
                        
                        option_labels = question_service._make_option_labels(len(options))
                        formatted_options = []
                        for i, option in enumerate(options):
                            formatted_options.append({
                                "label": option_labels[i],
                                "name": option['name'],
                                "count": option['count'],
                                "type": "brand_model",
                                "ids": option.get("ids"),
                            })
                        
                        question_data = {
                            "question": question_text,
                            "options": formatted_options,
                            "option_type": "brand_model"
                        }
                        
                        # æ›´æ–°å¯¹è¯çŠ¶æ€
                        conv_state.update_state(ConversationStateEnum.NEEDS_CHOICE)
                        conv_state.current_options = formatted_options
                        conv_state.option_type = "brand_model"
                        
                        # æ ¼å¼åŒ–æ¶ˆæ¯
                        message = question_service.format_question_message(question_data)
                        message = _prepend_selection_summary(message, locals().get("selection_summary"))
                        
                        conv_state.add_message("assistant", message)
                        
                        return ChatResponse(
                            message=message,
                            results=None,
                            options=formatted_options,
                            needs_choice=True,
                            session_id=session_id
                        )
                except Exception as e:
                    print(f"âš ï¸ Fallbacké€‰é¡¹ç”Ÿæˆå¤±è´¥: {str(e)}")
            
            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œå¼ºåˆ¶ç”Ÿæˆé€‰æ‹©é¢˜ï¼ˆå³ä½¿é€‰é¡¹ä¸å¤Ÿç†æƒ³ï¼‰
            if not best_options:
                print(f"âš ï¸ æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œå¼ºåˆ¶ç”Ÿæˆé€‰æ‹©é¢˜")
                # å¼ºåˆ¶ä»å±‚çº§è·¯å¾„ä¸­æå–é€‰é¡¹ï¼Œå³ä½¿åªæœ‰éƒ¨åˆ†åŒºåˆ†åº¦
                try:
                    hierarchy_options: Dict[str, set] = {}
                    for result in scored_results:
                        diagram = result.diagram
                        if diagram.hierarchy_path:
                            # å°è¯•æå–å±‚çº§è·¯å¾„ä¸­çš„ä¸åŒå±‚çº§ä½œä¸ºé€‰é¡¹
                            for i, level in enumerate(diagram.hierarchy_path):
                                if i > 0 and level and level != "ç”µè·¯å›¾" and len(level.strip()) > 1:
                                    # è·³è¿‡ç¬¬ä¸€ä¸ªå±‚çº§ï¼ˆé€šå¸¸æ˜¯"ç”µè·¯å›¾"ï¼‰
                                    level_clean = level.replace('*', '').strip()
                                    if not level_clean:
                                        continue
                                    # å·²çŸ¥ç±»å‹æ—¶è·³è¿‡ç±»å‹ç›¸å…³å±‚çº§ï¼Œé¿å…å†æ¬¡è¯¢é—®ç±»å‹
                                    type_keywords = ['ç”µè·¯å›¾', 'ä»ªè¡¨', 'ECU', 'æ•´è½¦', 'çº¿è·¯', 'é’ˆè„š', 'æ¨¡å—', 'æ¥çº¿']
                                    if type_already_specified and any(k in level_clean for k in type_keywords):
                                        continue
                                    # å·²çŸ¥å“ç‰Œæ—¶è·³è¿‡å“ç‰Œå±‚çº§ï¼Œé¿å…æŠŠå“ç‰Œå½“é€‰é¡¹
                                    if brand_tokens and any(bt and (bt in level_clean or level_clean in bt) for bt in brand_tokens):
                                        continue
                                    if level_clean:
                                        hierarchy_options.setdefault(level_clean, set()).add(diagram.id)
                    
                    # å¦‚æœæå–åˆ°é€‰é¡¹ï¼Œä½¿ç”¨å®ƒä»¬
                    if len(hierarchy_options) >= 2 and not type_already_specified:
                        options = [
                            {"name": name, "count": len(ids), "ids": sorted(ids)}
                            for name, ids in sorted(hierarchy_options.items(), key=lambda x: len(x[1]), reverse=True)[:max_options]
                        ]
                        options = _filter_out_noop_options(options, {r.diagram.id for r in scored_results})
                        if len(options) < 2:
                            options = []
                        
                        question_text = question_service._generate_question_text(
                            "type", total_found, context
                        )
                        
                        option_labels = question_service._make_option_labels(len(options))
                        formatted_options = []
                        for i, option in enumerate(options):
                            formatted_options.append({
                                "label": option_labels[i],
                                "name": option['name'],
                                "count": option['count'],
                                "type": "type",
                                "ids": option.get("ids"),
                            })
                        
                        question_data = {
                            "question": question_text,
                            "options": formatted_options,
                            "option_type": "type"
                        }
                        
                        conv_state.update_state(ConversationStateEnum.NEEDS_CHOICE)
                        conv_state.current_options = formatted_options
                        conv_state.option_type = "type"
                        
                        message = question_service.format_question_message(question_data)
                        message = _prepend_selection_summary(message, locals().get("selection_summary"))
                        conv_state.add_message("assistant", message)
                        
                        return ChatResponse(
                            message=message,
                            results=None,
                            options=formatted_options,
                            needs_choice=True,
                            session_id=session_id
                        )
                    elif len(hierarchy_options) >= 2:
                        # å¦‚æœç±»å‹å·²çŸ¥ï¼Œåˆ™å°†å±‚çº§é€‰é¡¹è§†ä¸ºç³»åˆ—/å‹å·é€‰é¡¹ç»§ç»­è¿½é—®
                        options = [
                            {"name": name, "count": len(ids), "ids": sorted(ids)}
                            for name, ids in sorted(hierarchy_options.items(), key=lambda x: len(x[1]), reverse=True)[:max_options]
                        ]
                        options = _filter_out_noop_options(options, {r.diagram.id for r in scored_results})
                        if len(options) < 2:
                            options = []

                        question_text = question_service._generate_question_text(
                            "brand_model", total_found, context
                        )

                        option_labels = question_service._make_option_labels(len(options))
                        formatted_options = []
                        for i, option in enumerate(options):
                            formatted_options.append({
                                "label": option_labels[i],
                                "name": option['name'],
                                "count": option['count'],
                                "type": "brand_model",
                                "ids": option.get("ids"),
                            })

                        question_data = {
                            "question": question_text,
                            "options": formatted_options,
                            "option_type": "brand_model"
                        }

                        conv_state.update_state(ConversationStateEnum.NEEDS_CHOICE)
                        conv_state.current_options = formatted_options
                        conv_state.option_type = "brand_model"

                        message = question_service.format_question_message(question_data)
                        message = _prepend_selection_summary(message, locals().get("selection_summary"))
                        conv_state.add_message("assistant", message)

                        return ChatResponse(
                            message=message,
                            results=None,
                            options=formatted_options,
                            needs_choice=True,
                            session_id=session_id
                        )
                    else:
                        # å¦‚æœè¿å±‚çº§è·¯å¾„éƒ½æå–ä¸åˆ°è¶³å¤Ÿçš„é€‰é¡¹ï¼Œè‡³å°‘åŸºäºæ–‡ä»¶åç”Ÿæˆé€‰é¡¹
                        print(f"âš ï¸ å±‚çº§è·¯å¾„æå–å¤±è´¥ï¼Œå°è¯•åŸºäºæ–‡ä»¶åç”Ÿæˆé€‰é¡¹")
                        file_name_options: Dict[str, set] = {}
                        for result in scored_results[:max_results * 2]:  # æ£€æŸ¥æ›´å¤šç»“æœä»¥æ‰¾åˆ°åŒºåˆ†åº¦
                            diagram = result.diagram
                            # ä»æ–‡ä»¶åä¸­æå–å…³é”®è¯ï¼ˆå»é™¤å“ç‰Œå’Œå¸¸è§è¯ï¼‰
                            file_name = diagram.file_name
                            # å°è¯•æå–æ–‡ä»¶åä¸­çš„å…³é”®éƒ¨åˆ†
                            if diagram.brand and diagram.brand in file_name:
                                # æå–å“ç‰Œåé¢çš„éƒ¨åˆ†
                                parts = file_name.split(diagram.brand, 1)
                                if len(parts) > 1:
                                    key_part = parts[1].split('.')[0].strip('_-. ')[:20]  # å–å‰20ä¸ªå­—ç¬¦
                                    if key_part and len(key_part) > 1:
                                        file_name_options.setdefault(key_part, set()).add(diagram.id)
                            
                            # æˆ–è€…ç›´æ¥ä½¿ç”¨æ–‡ä»¶åçš„ä¸€éƒ¨åˆ†
                            if not file_name_options:
                                # æå–æ–‡ä»¶åä¸­çš„å…³é”®è¯ï¼ˆå»é™¤æ‰©å±•åï¼‰
                                name_part = file_name.split('.')[0]
                                if len(name_part) > 5:
                                    # å–æ–‡ä»¶åçš„ä¸€éƒ¨åˆ†ä½œä¸ºé€‰é¡¹
                                    key_part = name_part[:15]
                                    file_name_options.setdefault(key_part, set()).add(diagram.id)
                        
                        if len(file_name_options) >= 2:
                            options = [
                                {"name": name, "count": len(ids), "ids": sorted(ids)}
                                for name, ids in sorted(file_name_options.items(), key=lambda x: len(x[1]), reverse=True)[:max_options]
                            ]
                            options = _filter_out_noop_options(options, {r.diagram.id for r in scored_results})
                            if len(options) < 2:
                                options = []
                            
                            question_text = f"æ‰¾åˆ°äº† {total_found} ä¸ªç›¸å…³ç»“æœã€‚è¯·é€‰æ‹©æ‚¨éœ€è¦çš„ç±»å‹ï¼š"
                            
                            option_labels = question_service._make_option_labels(len(options))
                            formatted_options = []
                            for i, option in enumerate(options):
                                formatted_options.append({
                                    "label": option_labels[i],
                                    "name": option['name'],
                                    "count": option['count'],
                                    "type": "type",
                                    "ids": option.get("ids"),
                                })
                            
                            question_data = {
                                "question": question_text,
                                "options": formatted_options,
                                "option_type": "type"
                            }
                            
                            conv_state.update_state(ConversationStateEnum.NEEDS_CHOICE)
                            conv_state.current_options = formatted_options
                            conv_state.option_type = "type"
                            
                            message = question_service.format_question_message(question_data)
                            message = _prepend_selection_summary(message, locals().get("selection_summary"))
                            conv_state.add_message("assistant", message)
                            
                            return ChatResponse(
                                message=message,
                                results=None,
                                options=formatted_options,
                                needs_choice=True,
                                session_id=session_id
                            )
                except Exception as e:
                    print(f"âš ï¸ å¼ºåˆ¶ç”Ÿæˆé€‰æ‹©é¢˜å¤±è´¥: {str(e)}")
                
                # å¦‚æœæ‰€æœ‰å¼ºåˆ¶ç”Ÿæˆæ–¹æ³•éƒ½å¤±è´¥ï¼Œè‡³å°‘ç”Ÿæˆä¸€ä¸ªåŸºäºç»“æœæ•°é‡çš„é€‰æ‹©é¢˜
                print(f"âš ï¸ æ‰€æœ‰å¼ºåˆ¶ç”Ÿæˆæ–¹æ³•éƒ½å¤±è´¥ï¼Œç”ŸæˆåŸºäºç»“æœçš„åˆ†ç»„é€‰æ‹©é¢˜")
                # å°†ç»“æœåˆ†æˆå‡ ç»„ï¼Œè®©ç”¨æˆ·é€‰æ‹©
                group_size = max(2, total_found // max_results)
                groups = []
                for i in range(0, min(total_found, max_results * 2), group_size):
                    group_results = scored_results[i:i+group_size]
                    if group_results:
                        # æå–è¿™ç»„ç»“æœçš„å…³é”®ç‰¹å¾
                        group_name = f"ç¬¬{i+1}-{min(i+group_size, total_found)}ä¸ªç»“æœ"
                        if group_results[0].diagram.brand:
                            group_name = f"{group_results[0].diagram.brand}ç›¸å…³"
                        groups.append({
                            "name": group_name,
                            "count": len(group_results),
                            "results": group_results
                        })
                
                if len(groups) >= 2:
                    question_text = f"æ‰¾åˆ°äº† {total_found} ä¸ªç›¸å…³ç»“æœã€‚è¯·é€‰æ‹©æ‚¨éœ€è¦çš„èŒƒå›´ï¼š"
                    
                    sliced_groups = groups[:max_options]
                    option_labels = question_service._make_option_labels(len(sliced_groups))
                    formatted_options = []
                    for i, group in enumerate(sliced_groups):
                        group_ids = [r.diagram.id for r in (group.get("results") or []) if getattr(r, "diagram", None)]
                        formatted_options.append({
                            "label": option_labels[i],
                            "name": group['name'],
                            "count": group['count'],
                            "type": "group",
                            "ids": group_ids,
                        })
                    # Drop no-op groups (ids == all_ids) and re-label
                    all_ids = {r.diagram.id for r in scored_results}
                    formatted_options = _filter_out_noop_options(formatted_options, all_ids)
                    option_labels = question_service._make_option_labels(len(formatted_options))
                    for i, o in enumerate(formatted_options):
                        o["label"] = option_labels[i]
                        o["count"] = len(o.get("ids") or [])
                    
                    question_data = {
                        "question": question_text,
                        "options": formatted_options,
                        "option_type": "group"
                    }
                    
                    conv_state.update_state(ConversationStateEnum.NEEDS_CHOICE)
                    conv_state.current_options = formatted_options
                    conv_state.option_type = "group"
                    # ä¿å­˜åˆ†ç»„ç»“æœä»¥ä¾¿åç»­ä½¿ç”¨
                    conv_state.grouped_results = groups
                    
                    message = question_service.format_question_message(question_data)
                    message = _prepend_selection_summary(message, locals().get("selection_summary"))
                    conv_state.add_message("assistant", message)
                    
                    return ChatResponse(
                        message=message,
                        results=None,
                        options=formatted_options,
                        needs_choice=True,
                        session_id=session_id
                    )
                
                # å¦‚æœè¿åˆ†ç»„éƒ½å¤±è´¥ï¼Œè¿”å›é”™è¯¯æç¤ºï¼ˆè¿™ç§æƒ…å†µåº”è¯¥å¾ˆå°‘è§ï¼‰
                error_message = f"æ‰¾åˆ°äº† {total_found} ä¸ªç›¸å…³ç»“æœï¼Œä½†æ— æ³•ç”Ÿæˆé€‰æ‹©é¢˜ã€‚è¯·å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯é‡æ–°æœç´¢ã€‚"
                conv_state.update_state(ConversationStateEnum.COMPLETED)
                conv_state.add_message("assistant", error_message)
                return ChatResponse(
                    message=error_message,
                    session_id=session_id
                )
    else:
        # ç»“æœâ‰¤5ä¸ªï¼Œç›´æ¥è¿”å›æ‰€æœ‰ç»“æœ
        print(f"âœ… ç»“æœæ•°({total_found}) <= max_results({max_results})ï¼Œç›´æ¥è¿”å›ç»“æœ")
        formatted_results = []
        for result in scored_results[:max_results]:
            formatted_results.append({
                "id": result.diagram.id,
                "file_name": result.diagram.file_name,
                "hierarchy_path": " -> ".join(result.diagram.hierarchy_path),
                "score": round(result.score, 2),
                "brand": result.diagram.brand,
                "model": result.diagram.model,
                "diagram_type": result.diagram.diagram_type
            })
        
        message = f"æ‰¾åˆ°äº† {total_found} ä¸ªç›¸å…³ç»“æœï¼š\n\n"
        for i, result in enumerate(formatted_results, 1):
            message += f"{i}. [ID: {result['id']}] {result['file_name']}\n"
            message += f"   è·¯å¾„: {result['hierarchy_path']}\n"
            if result['brand'] or result['model']:
                attrs = []
                if result['brand']:
                    attrs.append(f"å“ç‰Œ: {result['brand']}")
                if result['model']:
                    attrs.append(f"å‹å·: {result['model']}")
                if result['diagram_type']:
                    attrs.append(f"ç±»å‹: {result['diagram_type']}")
                if attrs:
                    message += f"   {', '.join(attrs)}\n"
            message += "\n"
        
        conv_state.update_state(ConversationStateEnum.COMPLETED)
        conv_state.add_message("assistant", message)
        
        return ChatResponse(
            message=message,
            results=formatted_results,
            needs_choice=False,
            session_id=session_id
        )
